{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Clouderizer is an end to end MLOps platform for businesses. Showcase \u00b6 Building a machine learning model to solve a real life business problem like Product Recommendation, Predicting Sales or Inventory, is only a small part of overall puzzle. Once built, model needs to be tested, deployed, integrated with business processes and systems, monitored, re-trained and re-deployed. Clouderizer Showcase helps to stitch together these stages in ML model lifecycle and deploy an end to end pipeline which allows enterprises to take their models from RnD & PoC stage to production and create value for their businesses.","title":"Overview"},{"location":"#showcase","text":"Building a machine learning model to solve a real life business problem like Product Recommendation, Predicting Sales or Inventory, is only a small part of overall puzzle. Once built, model needs to be tested, deployed, integrated with business processes and systems, monitored, re-trained and re-deployed. Clouderizer Showcase helps to stitch together these stages in ML model lifecycle and deploy an end to end pipeline which allows enterprises to take their models from RnD & PoC stage to production and create value for their businesses.","title":"Showcase"},{"location":"advanced_setup/aws/","text":"Clouderizer has in-built deep integration with AWS. You can link your Clouderizer account with your AWS account, which allows you to create, configure, manage and delete AWS resources (including spot instances) from Clouderizer console itself. Below are the steps to do so Steps \u00b6 Login to your AWS account (Sign Up in case you don\u2019t have one already) Click on the name of your account (it is located in the top right corner of the console). In the expanded drop-down list, select My Security Credentials. Click on Get Started with IAM Users button. On next screen, click Add user button to add a new user for Clouderizer. Give a username e.g. clouderizer_user and set Access type to Programmatic access. Press Next. Select option Attach existing policies directly . Then from list below search and select AdministratorAccess policy. Press Next. Confirm the settings and press Next. Now you should see the success message with Access key ID and Secret access key. Note down both the values (you will need to press Show to view Secret access key). Now login to your Clouderizer console and go to Settings->Cloud Settings. Cloud Settings Enter the Access Key ID and Secret Access Key from step 7 above, and press Update. AWS Settings Thats it. Your Clouderizer account is now integrated with your AWS account. You can now create projects with AWS SPOT instances, get best recommended bidding price for your configuration, start and stop those machines from Clouderizer console itself.","title":"Aws"},{"location":"advanced_setup/aws/#steps","text":"Login to your AWS account (Sign Up in case you don\u2019t have one already) Click on the name of your account (it is located in the top right corner of the console). In the expanded drop-down list, select My Security Credentials. Click on Get Started with IAM Users button. On next screen, click Add user button to add a new user for Clouderizer. Give a username e.g. clouderizer_user and set Access type to Programmatic access. Press Next. Select option Attach existing policies directly . Then from list below search and select AdministratorAccess policy. Press Next. Confirm the settings and press Next. Now you should see the success message with Access key ID and Secret access key. Note down both the values (you will need to press Show to view Secret access key). Now login to your Clouderizer console and go to Settings->Cloud Settings. Cloud Settings Enter the Access Key ID and Secret Access Key from step 7 above, and press Update. AWS Settings Thats it. Your Clouderizer account is now integrated with your AWS account. You can now create projects with AWS SPOT instances, get best recommended bidding price for your configuration, start and stop those machines from Clouderizer console itself.","title":"Steps"},{"location":"advanced_setup/gcp/","text":"Clouderizer can link with your GCP account to allow you run your projects seamlessly from within Clouderizer console itself. In case you have not already signed up for GCP account, you can enable GCP from any of your gmail account from here and get $300 free credit for first year. This credit is good enough for almost 1000 hours of Nvidia Tesla K80 GPUs!! You will need to setup your billing info here but Google won't charge you anything unless you run out of your free credits and give your explicit consent for charge. New GCP Users \u00b6 This section describes how to start Clouderizer with Google Compute environment using the available Cloud Launcher offering. This guide explains how to set up Google Cloud Platform (GCP) for your ML / AI projects. GCP Signup \u00b6 Signup for new GCP account from here . 2. Google asks for your credit card information when you sign up for the free trial (i) 3. Once the billing setup is done you will land to the console Requesting GPU Quota \u00b6 1. Click on \"IAM and admin\" and then \"Quotas\" 2. In \"Metric\" click on \"None\", then search \"GPU\" and select \"GPUs (all regions)\" 3. You need to upgrade your account to access GPU 4. Select the \"Compute Engine API\" and then click \"Edit Quotas\" as current GPU limit is \"0\" 5. Edit Quotas \u2192 Change to \u201c1\u201d. Enter some description for your quota request under \"request description\" Once you submit quota request, Google team will review and get back to you in email like below 6.GPU quota is generally credited within an hour time Now you can follow below instructions for setting up Google Service Account. Existing GCP Users (Google Service Account) \u00b6 A service account is a special type of Google account intended to represent a non-human user that needs to authenticate and be authorized to access data in Google APIs. Tip Prerequisite \u2022 Billing Setup done \u2022 Having Valid GPU Quota You can integrate your Clouderizer account with your GCP account via Service Account. One just need to specify the JSON file from his GCP account and can login into our Clouderizer. Next next few steps will expand how to setup account Login to your GCP Console: Once logged in go to \u201cAPIs & Services\u201d and select \u201cCredentials\u201d 2. In \u201cCredentials\u201d go to \u201cService account key\u201d 3.Click on \u201cNew service account\u201d 4.Type a \"Name\" for this service account. Under \"Key type,\" select \"JSON\" 5.Click on \u201cRole\u201d \u2192 \u201cCompute Engine\u201d. Select \u201cCompute Admin\u201d and \"Storage Admin\". These are the roles needed by Clouderizer to manage your projects. Click \"Create\", and the GCP Console will generate a JSON key (as a .json text file), prompt you to download the file to your computer\u201d Warning Store this JSON file securely, as it contains your private key (and this file is the only copy of that key) Login to Clouderizer 6.Now go to Settings \u2192 Cloud Setting \u2192 Google Service Account Credentials \u2192 Choose file\u2192 Upload the Json file \u2192 Upload GCP Credentials Success Thats it. Your Clouderizer account is now integrated with your GCP account. Understanding GCP service accounts!","title":"Gcp"},{"location":"advanced_setup/gcp/#new-gcp-users","text":"This section describes how to start Clouderizer with Google Compute environment using the available Cloud Launcher offering. This guide explains how to set up Google Cloud Platform (GCP) for your ML / AI projects.","title":"New GCP Users"},{"location":"advanced_setup/gcp/#gcp-signup","text":"Signup for new GCP account from here . 2. Google asks for your credit card information when you sign up for the free trial (i) 3. Once the billing setup is done you will land to the console","title":"GCP Signup"},{"location":"advanced_setup/gcp/#requesting-gpu-quota","text":"1. Click on \"IAM and admin\" and then \"Quotas\" 2. In \"Metric\" click on \"None\", then search \"GPU\" and select \"GPUs (all regions)\" 3. You need to upgrade your account to access GPU 4. Select the \"Compute Engine API\" and then click \"Edit Quotas\" as current GPU limit is \"0\" 5. Edit Quotas \u2192 Change to \u201c1\u201d. Enter some description for your quota request under \"request description\" Once you submit quota request, Google team will review and get back to you in email like below 6.GPU quota is generally credited within an hour time Now you can follow below instructions for setting up Google Service Account.","title":"Requesting GPU Quota"},{"location":"advanced_setup/gcp/#existing-gcp-users-google-service-account","text":"A service account is a special type of Google account intended to represent a non-human user that needs to authenticate and be authorized to access data in Google APIs. Tip Prerequisite \u2022 Billing Setup done \u2022 Having Valid GPU Quota You can integrate your Clouderizer account with your GCP account via Service Account. One just need to specify the JSON file from his GCP account and can login into our Clouderizer. Next next few steps will expand how to setup account Login to your GCP Console: Once logged in go to \u201cAPIs & Services\u201d and select \u201cCredentials\u201d 2. In \u201cCredentials\u201d go to \u201cService account key\u201d 3.Click on \u201cNew service account\u201d 4.Type a \"Name\" for this service account. Under \"Key type,\" select \"JSON\" 5.Click on \u201cRole\u201d \u2192 \u201cCompute Engine\u201d. Select \u201cCompute Admin\u201d and \"Storage Admin\". These are the roles needed by Clouderizer to manage your projects. Click \"Create\", and the GCP Console will generate a JSON key (as a .json text file), prompt you to download the file to your computer\u201d Warning Store this JSON file securely, as it contains your private key (and this file is the only copy of that key) Login to Clouderizer 6.Now go to Settings \u2192 Cloud Setting \u2192 Google Service Account Credentials \u2192 Choose file\u2192 Upload the Json file \u2192 Upload GCP Credentials Success Thats it. Your Clouderizer account is now integrated with your GCP account. Understanding GCP service accounts!","title":"Existing GCP Users (Google Service Account)"},{"location":"advanced_setup/kaggle/","text":"Tip Borrow from https://towardsdatascience.com/kaggle-on-google-colab-easiest-way-to-transfer-datasets-and-remote-bash-e54c64054faa Clouderizer has come up with an integration with official Kaggle CLI to address this issue. One just need to specify the names of Kaggle competitions or datasets at time of creating Clouderizer project. Clouderizer would then automatically download appropriate files and datasets, on any machine project is run, every time the project is run. Login to your Kaggle account and go to My Account -> API and Create New API Token. This will download your API token file, kaggle.json, on your machine. In case you don\u2019t have one already, sign up for a free Clouderizer account. Now login to your Clouderizer console and go to Settings -> Cloud Setting -> Kaggle Credentials. Choose kaggle.json file downloaded in step 1 and press Upload Kaggle Credentials. This will link your Kaggle account with Clouderizer, allowing you to auto download Kaggle datasets in your projects.","title":"Kaggle"},{"location":"cli/delete/","text":"To delete a project, cldz delete PROJECT_NAME . All the project related files / models / functions / invocation history will be destroyed. \u00b6","title":"cldz delete"},{"location":"cli/delete/#all-the-project-related-files-models-functions-invocation-history-will-be-destroyed","text":"","title":"All the project related files / models / functions / invocation history will be destroyed."},{"location":"cli/deploy/","text":"Deploy Notebooks \u00b6 Syntax \u00b6 cldz deploy -n python PATH_TO_JUPYTER_NOTEBOOK {REQUIREMENTS_TXT} -n flag tells cldz that deployment is of type notebook. Immediately after -n we follow up with the notebook kernel type. Following kernels are supported python R (coming soon) julia (coming soon) If no flag is given, cldz assumes the first argument is a python based jupyter notebook and second argument is a requirements file. Example \u00b6 cldz deploy -n python awesome-notebook.ipynb requirements.txt Async Endpoints \u00b6 Notebook deployments support Async serverless endpoints. This is useful as often Notebook execution is a long running operation. Async endpoints can be invoked via any http client like curl. Caller can specify a callback URL (webhook) in its http invocation under header X-callback-url . Serverless function immediately replies back with 202 request accepted response code and continue to execute the notebook. Once notebook execution is complete, the callback URL (webhook) is called with the result. In addition to this webhook, output of the invocation can also be viewed under Invocation History on Clouderizer console. Deploy Models \u00b6 Syntax \u00b6 cldz deploy -m MODEL_TYPE PATH_TO_H2O_MODEL_FILE -m flag tells cldz that deployment is of type model. Immediately after -m we follow up with the model type. Following model types are supported h2o.ai MOJO model PMML Python ONNX h2o / dai / pmml model deployment \u00b6 cldz deploy -m h2o PATH_TO_H2O_MODEL_FILE -m flag tells cldz that deployment is of type model. If -m is not specified, cldz assumes type as notebook. h2o in the above example can be replaced with dai / pmml. python model deployment \u00b6 Since python predict code needs to be executed. cldz deploy -m python PATH_TO_PYTHON_MODEL --predict predict.py PATH_TO_PYTHON_MODEL is usually the model file we load and score in predict.py . Pre-processing and Post-processing scripts \u00b6 Does your model requires preprocess / postprocess scripts at the time of scoring? Suppose you have a pmml model with both preprocess and postprocess files. Syntax is: cldz deploy -m pmml PATH_TO_PMML_FILE --preprocess preprocess.py --postprocess postprocess.py VERY IMPORTANT NOTE: Preprocess and postprocess flags are optional. Even mispelling the flags will deploy the model but without the mispelled arguments. Infra Configuration Flag \u00b6 --infra flag tells cldz the type of infra you want to run on. Clouderizer currently supports 3 infra types: standard - 2GB Memory highmemory - 6GB Memory gpu - 16GB GPU Memory, 30GB System Memory standard infra type is selected by default but if you want to start a project on gpu Note Tensorflow 2.4+ is supported for GPU. cldz start PROJECT_NAME --infra gpu Image Configuration Flag \u00b6 --image flag tells cldz the type of base image you want to run on. Clouderizer currently supports 3 image types: standard - python 3.8 tensorflow - python 3.8 + Tensorflow 2.4 torch - python 3.8 + Pytorch + Fastai + nbdev standard image type is selected by default cldz deploy my_notebook.ipynb --image torch","title":"cldz deploy"},{"location":"cli/deploy/#deploy-notebooks","text":"","title":"Deploy Notebooks"},{"location":"cli/deploy/#syntax","text":"cldz deploy -n python PATH_TO_JUPYTER_NOTEBOOK {REQUIREMENTS_TXT} -n flag tells cldz that deployment is of type notebook. Immediately after -n we follow up with the notebook kernel type. Following kernels are supported python R (coming soon) julia (coming soon) If no flag is given, cldz assumes the first argument is a python based jupyter notebook and second argument is a requirements file.","title":"Syntax"},{"location":"cli/deploy/#example","text":"cldz deploy -n python awesome-notebook.ipynb requirements.txt","title":"Example"},{"location":"cli/deploy/#async-endpoints","text":"Notebook deployments support Async serverless endpoints. This is useful as often Notebook execution is a long running operation. Async endpoints can be invoked via any http client like curl. Caller can specify a callback URL (webhook) in its http invocation under header X-callback-url . Serverless function immediately replies back with 202 request accepted response code and continue to execute the notebook. Once notebook execution is complete, the callback URL (webhook) is called with the result. In addition to this webhook, output of the invocation can also be viewed under Invocation History on Clouderizer console.","title":"Async Endpoints"},{"location":"cli/deploy/#deploy-models","text":"","title":"Deploy Models"},{"location":"cli/deploy/#syntax_1","text":"cldz deploy -m MODEL_TYPE PATH_TO_H2O_MODEL_FILE -m flag tells cldz that deployment is of type model. Immediately after -m we follow up with the model type. Following model types are supported h2o.ai MOJO model PMML Python ONNX","title":"Syntax"},{"location":"cli/deploy/#h2o-dai-pmml-model-deployment","text":"cldz deploy -m h2o PATH_TO_H2O_MODEL_FILE -m flag tells cldz that deployment is of type model. If -m is not specified, cldz assumes type as notebook. h2o in the above example can be replaced with dai / pmml.","title":"h2o / dai / pmml model deployment"},{"location":"cli/deploy/#python-model-deployment","text":"Since python predict code needs to be executed. cldz deploy -m python PATH_TO_PYTHON_MODEL --predict predict.py PATH_TO_PYTHON_MODEL is usually the model file we load and score in predict.py .","title":"python model deployment"},{"location":"cli/deploy/#pre-processing-and-post-processing-scripts","text":"Does your model requires preprocess / postprocess scripts at the time of scoring? Suppose you have a pmml model with both preprocess and postprocess files. Syntax is: cldz deploy -m pmml PATH_TO_PMML_FILE --preprocess preprocess.py --postprocess postprocess.py VERY IMPORTANT NOTE: Preprocess and postprocess flags are optional. Even mispelling the flags will deploy the model but without the mispelled arguments.","title":"Pre-processing and Post-processing scripts"},{"location":"cli/deploy/#infra-configuration-flag","text":"--infra flag tells cldz the type of infra you want to run on. Clouderizer currently supports 3 infra types: standard - 2GB Memory highmemory - 6GB Memory gpu - 16GB GPU Memory, 30GB System Memory standard infra type is selected by default but if you want to start a project on gpu Note Tensorflow 2.4+ is supported for GPU. cldz start PROJECT_NAME --infra gpu","title":"Infra Configuration Flag"},{"location":"cli/deploy/#image-configuration-flag","text":"--image flag tells cldz the type of base image you want to run on. Clouderizer currently supports 3 image types: standard - python 3.8 tensorflow - python 3.8 + Tensorflow 2.4 torch - python 3.8 + Pytorch + Fastai + nbdev standard image type is selected by default cldz deploy my_notebook.ipynb --image torch","title":"Image Configuration Flag"},{"location":"cli/describe/","text":"cldz ls gives the project status. To get more detailed description of the state of the project cldz describe PROJECT_NAME . Notebook describe output example, Model describe output example, In case of a deployed model, the description contains model access and scoring URLs.","title":"cldz describe"},{"location":"cli/loggedin/","text":"To check your user information, execute cldz loggedin in your terminal.","title":"Loggedin"},{"location":"cli/logout/","text":"By default, user will always be logged in with cli(after the first time setup). To logout, cldz logout","title":"Logout"},{"location":"cli/ls/","text":"You need to be logged in to use this command. Check Cli Setup section. cldz ls will give the list of all the deployed projects assosciated with the account.","title":"cldz ls"},{"location":"cli/miscellaneous/","text":"cldz loggedin \u00b6 To check your user information, execute cldz loggedin in your terminal. cldz logout \u00b6 By default, user will always be logged in with cli(after the first time setup). To logout, cldz logout","title":"Miscellaneous"},{"location":"cli/miscellaneous/#cldz-loggedin","text":"To check your user information, execute cldz loggedin in your terminal.","title":"cldz loggedin"},{"location":"cli/miscellaneous/#cldz-logout","text":"By default, user will always be logged in with cli(after the first time setup). To logout, cldz logout","title":"cldz logout"},{"location":"cli/setup/","text":"Clouderizer offers a python based CLI for creating, deploying and managing its projects. Pre-requisites \u00b6 System running MacOS / Ubuntu / Windows (with WSL) bash / sh / zsh terminal Python 3+ Pip for python3 Steps \u00b6 Install Clouderizer CLI by running the following command in the terminal pip install clouderizer Open a new terminal session for the changes to take effect. To test if cli was installed, type cldz . The cldz help section similar to the below image should show up. To use cli you need to be a Clouderizer user, cldz login is the command to authenticate yourself. Once cldz login is executed, \u2192 a browser window will open(if browser does not open, manually open the link as seen in image below). \u2192 Follow the Google authentication work flow to get yourself loggedin. If your are an existing user select the google account you are using with clouderizer. New users will be automatically signed up with Clouderizer via the same process. \u2192 After completing the google flow, you are now logged in with clouderizer and redirected to cli login authentication token page. \u2192 Copy the token and paste in the terminal. On successful login you should see the terminal output similar to below image. Cli is successfully setup and you are logged in.","title":"Setup"},{"location":"cli/setup/#pre-requisites","text":"System running MacOS / Ubuntu / Windows (with WSL) bash / sh / zsh terminal Python 3+ Pip for python3","title":"Pre-requisites"},{"location":"cli/setup/#steps","text":"Install Clouderizer CLI by running the following command in the terminal pip install clouderizer Open a new terminal session for the changes to take effect. To test if cli was installed, type cldz . The cldz help section similar to the below image should show up. To use cli you need to be a Clouderizer user, cldz login is the command to authenticate yourself. Once cldz login is executed, \u2192 a browser window will open(if browser does not open, manually open the link as seen in image below). \u2192 Follow the Google authentication work flow to get yourself loggedin. If your are an existing user select the google account you are using with clouderizer. New users will be automatically signed up with Clouderizer via the same process. \u2192 After completing the google flow, you are now logged in with clouderizer and redirected to cli login authentication token page. \u2192 Copy the token and paste in the terminal. On successful login you should see the terminal output similar to below image. Cli is successfully setup and you are logged in.","title":"Steps"},{"location":"cli/start/","text":"An existing project can be started or restarted with new configuration using cldz start . cldz start PROJECT_NAME One flag accepted by cldz start is the type of --infra you want to run on. Clouderizer currently supports 3 infra types:standard, highmemory and gpu. standard infra type is selected by default but if you want to start a project on gpu cldz start PROJECT_NAME --infra gpu","title":"cldz start"},{"location":"cli/stop/","text":"To stop a running project, cldz stop PROJECT_NAME . Stopping the projects will make the project URLs inaccessible.","title":"cldz stop"},{"location":"cli/update/","text":"Clouderizer cli gives the flexibility to update all the existing project components. However converting a project from model \u2192 notebook or notebook \u2192 model is not possible. cldz update updates project components. \u2192 update preprocess function \u00b6 cldz update PROJECT_NAME --preprocess preprocess.py If a preprocess function already exists in your project, it will be replaced by the function in this file preprocess.py. Else, new preprocess function will be created and enabled. \u2192 add dependencies to your project \u00b6 cldz update PROJECT_NAME --requirements requirements.txt \u2192 update model \u00b6 cldz update PROJECT_NAME --model NEW_MODEL_PATH \u2192 notebook update \u00b6 cldz update PROJECT_NAME --notebook NEW_NOTEBOOK_PATH Once enabled preprocess & postprocess functions can also be disabled. \u2192 disable postprocess function \u00b6 cldz update PROJECT_NAME --disable postprocess","title":"cldz update"},{"location":"cli/update/#-update-preprocess-function","text":"cldz update PROJECT_NAME --preprocess preprocess.py If a preprocess function already exists in your project, it will be replaced by the function in this file preprocess.py. Else, new preprocess function will be created and enabled.","title":"--&gt; update preprocess function"},{"location":"cli/update/#-add-dependencies-to-your-project","text":"cldz update PROJECT_NAME --requirements requirements.txt","title":"--&gt; add dependencies to your project"},{"location":"cli/update/#-update-model","text":"cldz update PROJECT_NAME --model NEW_MODEL_PATH","title":"--&gt; update model"},{"location":"cli/update/#-notebook-update","text":"cldz update PROJECT_NAME --notebook NEW_NOTEBOOK_PATH Once enabled preprocess & postprocess functions can also be disabled.","title":"--&gt; notebook update"},{"location":"cli/update/#-disable-postprocess-function","text":"cldz update PROJECT_NAME --disable postprocess","title":"--&gt; disable postprocess function"},{"location":"faq/colab_faq/","text":"Q1. My project was working fine so far. Today I am not able to access Jupyter Notebook or Terminal. I get Bad Gateway error. Google Colab notebooks have an idle timeout of 90 minutes and absolute timeout of 12 hours. This means, if user does not interact with his Google Colab notebook for more than 90 minutes, its instance is automatically terminated. Also, maximum lifetime of a Colab instance is 12 hours. Once instance dies abruptly, sometimes your Clouderizer console might not be able to auto update the status of your project and it can continue to show as running. When you try to access Jupyter or Terminal for this project, it errors out. Reloading the Clouderizer page or refreshing project data, should update project status correctly to Not Running. The project needs to be run again, using startup command, on Colab notebook, to allocate a fresh instance. Q2. When I run fast.ai project on Google Colab, check for cuda and cudnn is coming false. On your Colab notebook go to Runtime -> Change Runtime Type and verify if Hardware Accelerator is GPU. Q3. I used community template for fast.ai course and it works perfectly for dogsandcats dataset. How do I automatically download other datasets from fast.ai course to my project? You can add Kaggle dataset or direct URLs for downloading addtional datasets. See this article for Kaggle dataset. For URLs, go to your project settings -> WORKSPACE -> Load Data from, paste the new URLs here and press enter to add to list. Save this project. Now when this project is launched, all the Kaggle and URL datasets specified will be auto downloaded. Q3. Running Clouderizer command on Google Colab gives syntax error. Clouderizer command is a bash script. In order to execute this script on Colab notebook, we need to pre-pend ! before the script e.g. !wget -NS --content-disposition \" https://console.clouderizer.com/givemeinitsh/XXXXXX \" && bash ./clouderizer_init.sh","title":"Colab faq"},{"location":"getstarted/gdrive_setup/","text":"Once you login to your console, you should now see a prompt to link your Clouderizer account to Google Drive. Prompt to link Google Drive account with Clouderizer Alternatively, you can also go to Settings->Cloud Settings, and press \u201cAuthorize Access to Google Drive\u201d Cloud Settings for linking Google Drive account with Clouderizer Once you click this, it will take you to Google Login screen. Here you should login with the Google Account that you wish to link. Once you login, you will be presented with Authorization screen from Google as below Note This access is needed to make sure Clouderizer is able to offer 2-way sync between your Google Drive and machine where your projects are running. We don\u2019t use this access for any other purposes. At any point of time, you can go to Cloud Settings on Clouderizer console and revoke Google Drive access. You can revoke access directly from your Google Account settings here as well. In case you are still not comfortable giving API access from your Google Drive account, you can always signup and setup a fresh Google Account only for using with Clouderizer. Click \u201cAllow\u201d here. This should now take you back to Clouderizer console and you should see a success notification indicating that Google Drive integration was successful. Google Drive successfully configured Done. Your Google Drive will now show a new folder clouderizer Any new project that you create now, will automatically create a new sub-folder inside this. And all project folders will have code / data / out sub-folders as well, that correspond to code / data / out folders of your Clouderizer project. Here is an article that explains about these folders and how 2-way sync works between your machine and Clouderizer Drive. For our existing users, who have projects created before Google Drive integration, next time you run your project, its folder will automatically appear on Google Drive. And in case you don\u2019t see any data or out folder for your projects, you can always create them manually on Google Drive and upload data in them. They will automatically sync with your machine once you run your projects.","title":"Gdrive setup"},{"location":"getstarted/how_works/","text":"Tip Lets get a block diagram showing how data / code flows and show BYOC concept works.","title":"How works"},{"location":"getstarted/quick_start/","text":"Below are the steps to setup fast.ai course notebooks on Google Colab. Following pre-requisite, one time, steps are needed. Sign up and request access to Google Colab. Sign up for Clouderizer. Login to Clouderizer console. On first login, you will be prompted to link your Google Drive with Clouderizer. Follow on-screen instructions to do so. Now from Clouderizer console, go to Community Projects. Search for fast.ai template and clone it. On project wizard, select default options on all screens and save. Now every time you need to start fast.ai notebooks on google colab GPU, follow below 2 steps From Clouderizer console, press Start on fast.ai project created earlier. This will ask you to select the platform where you wish to run this project. Select \u201cGoogle Colab\u201d here. Select Google Colab as Platform to start This should create a Colab notebook for your project. Press \u201cLaunch Notebook Now\u201d. This will open the Colab notebook in a new tab. Run the code block in Colab notebook. Starting the project on Colab notebook Thats it! This will trigger an automated fast.ai course environment setup, latest code download and dogscats dataset download. You can go back to Clouderizer console and track the progress of this setup. Once setup is complete, project status becomes Running and Jupyter Notebook button becomes available. Clicking on Jupyter button, will open Jupyter notebook with fast.ai github code. Project Started IMPORTANT NOTE In case while working on JupyterLab or remote terminal, you get 502 Gateway Error, this is due to Colab instance getting terminated behind the scene. Please note that Google Colab instances are volatile. They get terminated every 12 hours, or can get terminated much sooner in case of high demand and idle sessions. While your Clouderizer project is running on Colab notebook, make sure you don\u2019t close Colab notebook tab. Preferably, keep it in foreground on one of your secondary displays. This helps in preventing pre-mature shutdown of Colab instance.","title":"Quick start"},{"location":"showcase/deploymodels/","text":"title: Deployment description: Deployment Infra Type: \"GPU, High Memory, Standard Memory\" Once clouderizer project is created and fully configured, we are all set for deployment. We can trigger deployment from CLI or from our web console. For CLI instructions please refer here For deploying from web console, go to your project page and press the Deploy button on top right. Figure 1 - Deploy model button Following deployment options are available Standard Memory config - 2GB Memory High Memory config - 6GB Memory GPU Config - 16GB GPU Memory, 30GB System Memory Select the desired infra option and press OK. This will trigger start the deployment process, where your project is bundled in a container and pushed on Clouderizer Serverless infrastructure. Deployment Status \u00b6 Once deployment is started using any of the methods above, status on the project page updates to indicate deployment is in progress. We can see details about the progress from bottom left portion of screen. Figure 8 - Deployment in progress When deployment is complete, status changes to Running state and we can see deployed project's URL in the bottom left portion of screen. This URL can be used for invoking the serverless endpoint. More details on scoring can be found here and here . Figure 9 - Deployment complete Scalable Serverless Deployment \u00b6 Clouderizer offers a scalable serverless infrastructure, which means it auto scales your deployment to multiple replicas and nodes in case of high load and it auto scales down to 0 in case of no traffic. Billing for Clouderizer infra is per-second execution time, which significantly saves on idle capacity charges.","title":"Deploy"},{"location":"showcase/deploymodels/#deployment-status","text":"Once deployment is started using any of the methods above, status on the project page updates to indicate deployment is in progress. We can see details about the progress from bottom left portion of screen. Figure 8 - Deployment in progress When deployment is complete, status changes to Running state and we can see deployed project's URL in the bottom left portion of screen. This URL can be used for invoking the serverless endpoint. More details on scoring can be found here and here . Figure 9 - Deployment complete","title":"Deployment Status"},{"location":"showcase/deploymodels/#scalable-serverless-deployment","text":"Clouderizer offers a scalable serverless infrastructure, which means it auto scales your deployment to multiple replicas and nodes in case of high load and it auto scales down to 0 in case of no traffic. Billing for Clouderizer infra is per-second execution time, which significantly saves on idle capacity charges.","title":"Scalable Serverless Deployment"},{"location":"showcase/introduction/","text":"title: Introduction description: Supported Models: \"PMML, ONNX, Python, H2O.ai, Parameterized Jupyter Notebooks\", Serverless Function Type: \"GPU, High Memory\" Journey of a ML model from RnD stage (with a bunch of Data Scientists) to a fully managed production stage involves going through mulitple stages and touching multiple teams. Some of these stages are Deploying the model Testing the model (with new real life data, validation by other teams) Integrating the model with existing business application Monitoring and analysing the model performance with Ground Truth feedback Re-Training the model (with new data), once performance drops Re-deploying the new model version Repeat from Step 4 Clouderizer Showcase helps teams through each of these stages and setup and deploy an automated pipeline for ML models. Showcase supports deploying and managing the following Parameterized Jupyter Notebooks \u00b6 Clouderizer allows you to publish your local Jupyter Notebooks to cloud as Serverless Functions. Once deployed these serverless functions can be invoked using a simple curl command. Moreover, Clouderizer supports parameterized Notebooks using Papermill . We can tag any cell in our notebook as parameter before deploying. Then while invoking the serverless function, we can pass any variable value in the http request as parameters. As of now only Python kernel are supported in Jupyter Notebooks. R and Julia based kernel support is in works. In case you wish early access to this, please reach out to us at info@clouderizer.com H2O MOJO models \u00b6 H2O.ai helps businesses to create machine learning models to extract insights from their data, without having in-house expertise in designing and tuning such models. It is one of the most popular open source AutoML platforms helping Citizen Data scientists import their business data and easily create highly effective machine learning models from them. H2O.ai AutoML libraries can be used in Python or R. H2O also offers an advanced AutoML console called Driverless AI. H2O.ai models can be exported in a Java executable format called MOJO. You can upload these MOJO models into Clouderizer Showcase and deploy them. Clouderizer Showcase has deep integration with H2O MOJO scoring engine, such that, it can parse and score the models without requiring any custom code for pre-processing or prediction. Here is an example illustrating MOJO model deployment. PMML models \u00b6 PMML PMML stands for \"Predictive Model Markup Language\". It is the de facto standard to represent predictive solutions. A PMML file may contain a myriad of data transformations (pre- and post-processing) as well as one or more predictive models. Analytics platforms like Alteryx, RapidMiner, SaS, Dataiku have a direct way of exporting trained models in PMML. Models built using open source Python libs like sklearn, xgboost, lightGBM, etc, can also be exported to PMML using libraries like nyoka . You can upload these PMML models into Clouderizer Showcase and deploy them. Clouderizer Showcase has deep integration with PMML scoring engine, such that, it can parse and score the models without requiring any custom code for pre-processing or prediction. Here is an example illustrating PMML model deployment. Python Pickle objects \u00b6 One of the most common way of saving a python model is using Python object serialization or pickling. These models are stored on disk as a pickle file. Pickle files can be loaded back from disk to recreate the python objects or the ML model. This python object serialization allows us to save models created using common python ML and DL frameworks like Tensorflow, PyTorch, Scikit-learn, etc. You can upload these pickle file in Clouderizer Showcase and deploy them. While deploying python pickle models, we need to specify a prediction code snippet, which loads the model and does the prediction. You might also need to specify a pre-processing code snippet as well, depending upon your project. Here is an example illustrating python pickle model deployment. ONNX models (coming soon) \u00b6 ONNX is an open format built to represent machine learning models. While PMML standard tries to come up with a generic standard for Machine Learning models, ONNX aims to do the same for both Machine Learning as well as Deep Learning models as well.","title":"Introduction"},{"location":"showcase/introduction/#parameterized-jupyter-notebooks","text":"Clouderizer allows you to publish your local Jupyter Notebooks to cloud as Serverless Functions. Once deployed these serverless functions can be invoked using a simple curl command. Moreover, Clouderizer supports parameterized Notebooks using Papermill . We can tag any cell in our notebook as parameter before deploying. Then while invoking the serverless function, we can pass any variable value in the http request as parameters. As of now only Python kernel are supported in Jupyter Notebooks. R and Julia based kernel support is in works. In case you wish early access to this, please reach out to us at info@clouderizer.com","title":"Parameterized Jupyter Notebooks"},{"location":"showcase/introduction/#h2o-mojo-models","text":"H2O.ai helps businesses to create machine learning models to extract insights from their data, without having in-house expertise in designing and tuning such models. It is one of the most popular open source AutoML platforms helping Citizen Data scientists import their business data and easily create highly effective machine learning models from them. H2O.ai AutoML libraries can be used in Python or R. H2O also offers an advanced AutoML console called Driverless AI. H2O.ai models can be exported in a Java executable format called MOJO. You can upload these MOJO models into Clouderizer Showcase and deploy them. Clouderizer Showcase has deep integration with H2O MOJO scoring engine, such that, it can parse and score the models without requiring any custom code for pre-processing or prediction. Here is an example illustrating MOJO model deployment.","title":"H2O MOJO models"},{"location":"showcase/introduction/#pmml-models","text":"PMML PMML stands for \"Predictive Model Markup Language\". It is the de facto standard to represent predictive solutions. A PMML file may contain a myriad of data transformations (pre- and post-processing) as well as one or more predictive models. Analytics platforms like Alteryx, RapidMiner, SaS, Dataiku have a direct way of exporting trained models in PMML. Models built using open source Python libs like sklearn, xgboost, lightGBM, etc, can also be exported to PMML using libraries like nyoka . You can upload these PMML models into Clouderizer Showcase and deploy them. Clouderizer Showcase has deep integration with PMML scoring engine, such that, it can parse and score the models without requiring any custom code for pre-processing or prediction. Here is an example illustrating PMML model deployment.","title":"PMML models"},{"location":"showcase/introduction/#python-pickle-objects","text":"One of the most common way of saving a python model is using Python object serialization or pickling. These models are stored on disk as a pickle file. Pickle files can be loaded back from disk to recreate the python objects or the ML model. This python object serialization allows us to save models created using common python ML and DL frameworks like Tensorflow, PyTorch, Scikit-learn, etc. You can upload these pickle file in Clouderizer Showcase and deploy them. While deploying python pickle models, we need to specify a prediction code snippet, which loads the model and does the prediction. You might also need to specify a pre-processing code snippet as well, depending upon your project. Here is an example illustrating python pickle model deployment.","title":"Python Pickle objects"},{"location":"showcase/introduction/#onnx-models-coming-soon","text":"ONNX is an open format built to represent machine learning models. While PMML standard tries to come up with a generic standard for Machine Learning models, ONNX aims to do the same for both Machine Learning as well as Deep Learning models as well.","title":"ONNX models (coming soon)"},{"location":"showcase/quickstart/","text":"Clouderizer CLI \u00b6 Pre-requisites \u00b6 System running MacOS / Ubuntu / Windows (with WSL) bash / sh / zsh terminal Python 3+ Pip for python3 Steps \u00b6 Install Clouderizer CLI by running the following command in the terminal pip install clouderizer Login into your Clouderizer account using following command and following on-screen instructions cldz login Now cd into the directory where your notebook (say awesome-notebook.ipynb) resides. In case your notebook needs some specific libraries, put them in requirements.txt. Run the following command to deploy it on Clouderizer as Serverless function. Use \u2013infra flag to deploy it as a GPU function. cldz deploy awesome-notebook.ipynb requirements.txt --infra GPU This command should push your notebook to cloud as serverless function and give you an http endpoint for the notebook. Something like this NOTEBOOK AYSNC URL: https://serverless.clouderizer.com/async-function/awesome-notebook-func Congratulations!! Your notebook is now transformed into a serverless function. You can invoke your notebook using curl command curl -i -X POST -F param=XYZ https://serverless.clouderizer.com/async-function/awesome-notebook-func Web Console \u00b6 Pre-requisites \u00b6 System with a modern browser like Chrome / FireFox / Safari / Edge curl / postman / or any other http client Steps \u00b6 Sign up for a Clouderizer account from here and login into the web console from here. Click on New Project to create your project and give it a name. Select type as Notebook and press Next. Browse your notebook file (awesome-notebook.ipynb). In case your notebook needs some specific libraries, upload your requirements.txt file as well. Press Finish. This will upload your notebook and create the project. Press Deploy from top right corner of the screen. Select GPU from the infra type to deploy your notebook as a GPU function. This should deploy your notebook as a serverless GPU function and you should see an http endpoint for your function in the status box in bottom left. Congratulations!! Your notebook is now transformed into a serverless function. You can invoke your notebook using curl command curl -i -X POST -F param=XYZ https://serverless.clouderizer.com/function/async/awesome-notebook-func","title":"Quick Start"},{"location":"showcase/quickstart/#clouderizer-cli","text":"","title":"Clouderizer CLI"},{"location":"showcase/quickstart/#pre-requisites","text":"System running MacOS / Ubuntu / Windows (with WSL) bash / sh / zsh terminal Python 3+ Pip for python3","title":"Pre-requisites"},{"location":"showcase/quickstart/#steps","text":"Install Clouderizer CLI by running the following command in the terminal pip install clouderizer Login into your Clouderizer account using following command and following on-screen instructions cldz login Now cd into the directory where your notebook (say awesome-notebook.ipynb) resides. In case your notebook needs some specific libraries, put them in requirements.txt. Run the following command to deploy it on Clouderizer as Serverless function. Use \u2013infra flag to deploy it as a GPU function. cldz deploy awesome-notebook.ipynb requirements.txt --infra GPU This command should push your notebook to cloud as serverless function and give you an http endpoint for the notebook. Something like this NOTEBOOK AYSNC URL: https://serverless.clouderizer.com/async-function/awesome-notebook-func Congratulations!! Your notebook is now transformed into a serverless function. You can invoke your notebook using curl command curl -i -X POST -F param=XYZ https://serverless.clouderizer.com/async-function/awesome-notebook-func","title":"Steps"},{"location":"showcase/quickstart/#web-console","text":"","title":"Web Console"},{"location":"showcase/quickstart/#pre-requisites_1","text":"System with a modern browser like Chrome / FireFox / Safari / Edge curl / postman / or any other http client","title":"Pre-requisites"},{"location":"showcase/quickstart/#steps_1","text":"Sign up for a Clouderizer account from here and login into the web console from here. Click on New Project to create your project and give it a name. Select type as Notebook and press Next. Browse your notebook file (awesome-notebook.ipynb). In case your notebook needs some specific libraries, upload your requirements.txt file as well. Press Finish. This will upload your notebook and create the project. Press Deploy from top right corner of the screen. Select GPU from the infra type to deploy your notebook as a GPU function. This should deploy your notebook as a serverless GPU function and you should see an http endpoint for your function in the status box in bottom left. Congratulations!! Your notebook is now transformed into a serverless function. You can invoke your notebook using curl command curl -i -X POST -F param=XYZ https://serverless.clouderizer.com/function/async/awesome-notebook-func","title":"Steps"},{"location":"showcase/managemodels/modelversions/","text":"In beta. Documentation update pending.","title":"Modelversions"},{"location":"showcase/managemodels/monitormodels/","text":"Figure 1 - Analytics Clouderizer Showcase offers an analytics dashboard to monitor deployed model performance and other statistics. This dashboard can be accessed from Analytics button from Showcase project page. Analytics Dashboard offers following Charts \u00b6 Top of the Analytics dashboard shows statistical charts showing historical trends. Request Count \u00b6 This shows rate of requests hitting Showcase project deployment server. This is a good indicator for monitoring request traffic hitting the model server. Prediction Response Time \u00b6 This shows trend for average response time taken by the model to make prediction. It is a good indicator for monitoring model compute performance. Feedback \u00b6 This shows trend for positive and negative feedback from users or integrated applications for model prediction. This again is a good indicator to keep a track on model performance and detect any deterioration in model accuracy. Historical Request List \u00b6 Showcase stores each requests hitting the model server in a scalable time series storage. This list is available to us from Analytics dashboard. Important columns from this list are Timestamp - Time of request Response Time - Time taken for model to make prediction Feedback - User feedback on the model prediction Prediction - whether prediction was done successfully or not. In case some error occured in making the prediction, this staus changes to Error. View Request Details \u00b6 Figure 2 - View Request Details Clicking on More details button on individual row from the list, shows the details of input parameters sent for the request and corresponding output scored by the model. Download CSV \u00b6 Figure 3 - Download CSV The historical request list can be downloaded as a CSV using the Download CSV button on the top right corner of the Analytics page. This is very useful to prepare an addendum to the labelled dataset for re-training the model.","title":"Monitor Models"},{"location":"showcase/managemodels/monitormodels/#charts","text":"Top of the Analytics dashboard shows statistical charts showing historical trends.","title":"Charts"},{"location":"showcase/managemodels/monitormodels/#request-count","text":"This shows rate of requests hitting Showcase project deployment server. This is a good indicator for monitoring request traffic hitting the model server.","title":"Request Count"},{"location":"showcase/managemodels/monitormodels/#prediction-response-time","text":"This shows trend for average response time taken by the model to make prediction. It is a good indicator for monitoring model compute performance.","title":"Prediction Response Time"},{"location":"showcase/managemodels/monitormodels/#feedback","text":"This shows trend for positive and negative feedback from users or integrated applications for model prediction. This again is a good indicator to keep a track on model performance and detect any deterioration in model accuracy.","title":"Feedback"},{"location":"showcase/managemodels/monitormodels/#historical-request-list","text":"Showcase stores each requests hitting the model server in a scalable time series storage. This list is available to us from Analytics dashboard. Important columns from this list are Timestamp - Time of request Response Time - Time taken for model to make prediction Feedback - User feedback on the model prediction Prediction - whether prediction was done successfully or not. In case some error occured in making the prediction, this staus changes to Error.","title":"Historical Request List"},{"location":"showcase/managemodels/monitormodels/#view-request-details","text":"Figure 2 - View Request Details Clicking on More details button on individual row from the list, shows the details of input parameters sent for the request and corresponding output scored by the model.","title":"View Request Details"},{"location":"showcase/managemodels/monitormodels/#download-csv","text":"Figure 3 - Download CSV The historical request list can be downloaded as a CSV using the Download CSV button on the top right corner of the Analytics page. This is very useful to prepare an addendum to the labelled dataset for re-training the model.","title":"Download CSV"},{"location":"showcase/managemodels/monitornotebooks/","text":"Clouderizer Showcase offers Invocation History for all Notebook projects Invocations \u00b6 Showcase stores each requests hitting the serverless function in a scalable time series storage. This list is available to us from Invocations . Important columns from this list are Timestamp - Time of request Input - # of input parameters for the request Output - # of output parameters returned for the request Status - Whether notebook execution complete without any errors. In case of errors, detail of error is shown under details. View Request Details \u00b6 Clicking on More details button on individual row from the list, shows the details of input parameters sent for the request and corresponding output generated. All artifacts in http requests (files and fields) are archived in Invocation history. Similarly any output generated by the notebook is also archived here. These files can be downloaded from this view. In addition executed notebook is also avialable for viewing and download from here.","title":"Monitor Notebooks"},{"location":"showcase/managemodels/monitornotebooks/#invocations","text":"Showcase stores each requests hitting the serverless function in a scalable time series storage. This list is available to us from Invocations . Important columns from this list are Timestamp - Time of request Input - # of input parameters for the request Output - # of output parameters returned for the request Status - Whether notebook execution complete without any errors. In case of errors, detail of error is shown under details.","title":"Invocations"},{"location":"showcase/managemodels/monitornotebooks/#view-request-details","text":"Clicking on More details button on individual row from the list, shows the details of input parameters sent for the request and corresponding output generated. All artifacts in http requests (files and fields) are archived in Invocation history. Similarly any output generated by the notebook is also archived here. These files can be downloaded from this view. In addition executed notebook is also avialable for viewing and download from here.","title":"View Request Details"},{"location":"showcase/managemodels/triggerretraining/","text":"In beta. Documentation update pending","title":"Triggerretraining"},{"location":"showcase/managemodels/updatemodels/","text":"Model projects can be updated with new model files, preprocessing, prediction, post processing code, at any time, even if they are currently deployed and live. Clouderizer ensures rolling update with neglegible downtime. Tip For CLI instructions for updating models in a deployment, please refer here Updating Model File \u00b6 Figure 1 - Update Model File Model file can be uploaded using Update Model button on the Showcase project page under Model block. Updating Pre-processing Code \u00b6 Pre-processing code can be updated any time by going to the code editor using Pre-processing Code button under Pre-processing block. We can make our changes, test them by running it. Once finalized, we should Save and Upload the updated pre-processing code. Project needs to be re-deployed for the new changes to take effect. Updating Prediction Code \u00b6 Prediction code can be updated any time by going to the code editor using Prediction Code button under Model block. We can make our changes, test them by running it. Once finalized, we should Save and Upload the updated prediction code. Project needs to be re-deployed for the new changes to take effect.","title":"Update Models"},{"location":"showcase/managemodels/updatemodels/#updating-model-file","text":"Figure 1 - Update Model File Model file can be uploaded using Update Model button on the Showcase project page under Model block.","title":"Updating Model File"},{"location":"showcase/managemodels/updatemodels/#updating-pre-processing-code","text":"Pre-processing code can be updated any time by going to the code editor using Pre-processing Code button under Pre-processing block. We can make our changes, test them by running it. Once finalized, we should Save and Upload the updated pre-processing code. Project needs to be re-deployed for the new changes to take effect.","title":"Updating Pre-processing Code"},{"location":"showcase/managemodels/updatemodels/#updating-prediction-code","text":"Prediction code can be updated any time by going to the code editor using Prediction Code button under Model block. We can make our changes, test them by running it. Once finalized, we should Save and Upload the updated prediction code. Project needs to be re-deployed for the new changes to take effect.","title":"Updating Prediction Code"},{"location":"showcase/managemodels/updatenotebooks/","text":"Notebook projects can be updated with newer versions of notebook at any time. Clouderizer ensures rolling update with neglegible downtime. Tip For CLI instructions for updating notebooks in a deployment, please refer here Steps \u00b6 Login to the console and go to the project page. Press Update button from the central block. Upload the new notebook (and if needed new requirements.txt) file and press Finish. Once upload is completed, notebook is updated in the project. In order to push this new update to your existing deployment, press Deploy button from top right corner.","title":"Update Notebooks"},{"location":"showcase/managemodels/updatenotebooks/#steps","text":"Login to the console and go to the project page. Press Update button from the central block. Upload the new notebook (and if needed new requirements.txt) file and press Finish. Once upload is completed, notebook is updated in the project. In order to push this new update to your existing deployment, press Deploy button from top right corner.","title":"Steps"},{"location":"showcase/scoremodels/restfulapis/","text":"Once Showcase project is deployed successfully, Clouderizer exposes REST endpoints for it. These REST endpoints can be called by any external application to score their data using this model. REST endpoint URL is available in the bottom left screen of a running project Figure 1 - REST Enpoint URL Notebooks \u00b6 Async Endpoints \u00b6 Notebook deployments support Async serverless endpoints. This is useful as often Notebook execution is a long running operation. Async endpoints can be invoked via any http client like curl. Caller can specify a callback URL (webhook) in its http invocation using http header X-callback-url . Serverless function immediately replies back with 202 request accepted response code and continue to execute the notebook. Once notebook execution is complete, the callback URL (webhook) is called with the result. In addition to this webhook, output of the invocation can also be viewed under Invocation History on Clouderizer console. Models \u00b6 Following are the handlers available model deployment REST endpoint /predict \u00b6 This is the api which allows any external application to submit input and get back scored output. Showcase console offers sample code for invoking this handler in Node.js, Python, Java and C#. Figure 2 - REST Sample Code Method Type \u00b6 POST Input Payload \u00b6 Model input need to be submitted as a form payload with field name csv. Value of this field should be a comman separated values of all model inputs (order of inputs should be exactly same as order defined in Model project page) Output Payload \u00b6 Following is the convention for output for various data types Integer, Whole and Text \u00b6 Output is an array of all output variables defined in Model project page under Output. {\" \": ,..., \"responseTime\":\" \", \"success\":\"true\"} Enums \u00b6 Enum outputs are returned in json format with following schema {\"label\": , \"classprobabilities\":[ , ,...], \"responseTime\":\" \", \"success\",\" \"} Errors \u00b6 /feedback \u00b6 This is the api using which an external application or end user can give feedback about the model performance. Method Type \u00b6 POST Input Payload \u00b6 Output Payload \u00b6 Errors \u00b6","title":"RESTful APIs"},{"location":"showcase/scoremodels/restfulapis/#notebooks","text":"","title":"Notebooks"},{"location":"showcase/scoremodels/restfulapis/#async-endpoints","text":"Notebook deployments support Async serverless endpoints. This is useful as often Notebook execution is a long running operation. Async endpoints can be invoked via any http client like curl. Caller can specify a callback URL (webhook) in its http invocation using http header X-callback-url . Serverless function immediately replies back with 202 request accepted response code and continue to execute the notebook. Once notebook execution is complete, the callback URL (webhook) is called with the result. In addition to this webhook, output of the invocation can also be viewed under Invocation History on Clouderizer console.","title":"Async Endpoints"},{"location":"showcase/scoremodels/restfulapis/#models","text":"Following are the handlers available model deployment REST endpoint","title":"Models"},{"location":"showcase/scoremodels/restfulapis/#predict","text":"This is the api which allows any external application to submit input and get back scored output. Showcase console offers sample code for invoking this handler in Node.js, Python, Java and C#. Figure 2 - REST Sample Code","title":"/predict"},{"location":"showcase/scoremodels/restfulapis/#method-type","text":"POST","title":"Method Type"},{"location":"showcase/scoremodels/restfulapis/#input-payload","text":"Model input need to be submitted as a form payload with field name csv. Value of this field should be a comman separated values of all model inputs (order of inputs should be exactly same as order defined in Model project page)","title":"Input Payload"},{"location":"showcase/scoremodels/restfulapis/#output-payload","text":"Following is the convention for output for various data types","title":"Output Payload"},{"location":"showcase/scoremodels/restfulapis/#integer-whole-and-text","text":"Output is an array of all output variables defined in Model project page under Output. {\" \": ,..., \"responseTime\":\" \", \"success\":\"true\"}","title":"Integer, Whole and Text"},{"location":"showcase/scoremodels/restfulapis/#enums","text":"Enum outputs are returned in json format with following schema {\"label\": , \"classprobabilities\":[ , ,...], \"responseTime\":\" \", \"success\",\" \"}","title":"Enums"},{"location":"showcase/scoremodels/restfulapis/#errors","text":"","title":"Errors"},{"location":"showcase/scoremodels/restfulapis/#feedback","text":"This is the api using which an external application or end user can give feedback about the model performance.","title":"/feedback"},{"location":"showcase/scoremodels/restfulapis/#method-type_1","text":"POST","title":"Method Type"},{"location":"showcase/scoremodels/restfulapis/#input-payload_1","text":"","title":"Input Payload"},{"location":"showcase/scoremodels/restfulapis/#output-payload_1","text":"","title":"Output Payload"},{"location":"showcase/scoremodels/restfulapis/#errors_1","text":"","title":"Errors"},{"location":"showcase/scoremodels/scoringui/bulkscoring/","text":"Figure 1 - Download CSV Template In case we wish to make bulk predictions on our model, Scoring UI gives a very easy to use and intuitive interface to do that. Testers or users need not write any code or draft curl commands. They can just upload a CSV with a list of inputs and get back the output in one shot. Figure 2 - Editing CSV We can download a template CSV by pressing button #1 from Fig.1 above. This csv can be opened and edited in any text editor or spreadsheet editor. It contains the header rows. We can now insert rows of input data, save the file and upload it using the button #2 in Fig. 1 above. Figure 3 - Review Inputs Once uploaded, showcase allows us to review the bulk input rows and make corrections if needed. Press Save once all review changes are done. And then press Submit to trigger bulk prediction on this data. This will send this batch request to the deployed Showcase project, predict each individual rows from the project model and return back the scored data. Once output is ready, we can press View Results to view them. Figure 4 - View Results This opens up the list view with all input rows. All output columns are appended at the end. There is one additional column for feedback. User can give thumbs up or down for the individual line predictions here.","title":"Bulk Scoring"},{"location":"showcase/scoremodels/scoringui/customizations/","text":"Figure 1 - Scoring UI Customization Elements Showcase offers following customization and branding options for the Scoring UI Title and Description \u00b6 Figure 2 - Scoring UI Title & Description Project title and description can be configured from Showcase project page. Clicking on the Title name allows to modify it. Similarly clicking on the description text, allows us to modify it. Once modified, we can press Save button to commit it. Banner Image \u00b6 Figure 3 - Scoring UI Banner Image Banner image can be updated from Project page -> Settings. We can upload and save the banner image. Enum Output Images \u00b6 Figure 4 - Scoring UI Output Icons Figure 5 - Scoring UI Output Icons Output on the Scoring UI can be complimented with intuitive icons / images / gifs to offer an impressive presentation. This can be configured from Showcase project page -> Output -> Configure. All output variables of type Enums have an additional button Configure in their row (Fig 4). Pressing configure, gives option to select an image and specify a text (punch line) for each Enum state (Fig 5). Images can be either uploaded or selected from a builtin gallery. Once configured, save the setting. All the above customization options, once configured, get loaded by Scoring UI on next browser refresh.","title":"Customizations"},{"location":"showcase/scoremodels/scoringui/customizations/#title-and-description","text":"Figure 2 - Scoring UI Title & Description Project title and description can be configured from Showcase project page. Clicking on the Title name allows to modify it. Similarly clicking on the description text, allows us to modify it. Once modified, we can press Save button to commit it.","title":"Title and Description"},{"location":"showcase/scoremodels/scoringui/customizations/#banner-image","text":"Figure 3 - Scoring UI Banner Image Banner image can be updated from Project page -> Settings. We can upload and save the banner image.","title":"Banner Image"},{"location":"showcase/scoremodels/scoringui/customizations/#enum-output-images","text":"Figure 4 - Scoring UI Output Icons Figure 5 - Scoring UI Output Icons Output on the Scoring UI can be complimented with intuitive icons / images / gifs to offer an impressive presentation. This can be configured from Showcase project page -> Output -> Configure. All output variables of type Enums have an additional button Configure in their row (Fig 4). Pressing configure, gives option to select an image and specify a text (punch line) for each Enum state (Fig 5). Images can be either uploaded or selected from a builtin gallery. Once configured, save the setting. All the above customization options, once configured, get loaded by Scoring UI on next browser refresh.","title":"Enum Output Images"},{"location":"showcase/scoremodels/scoringui/formbasedscoring/","text":"Most simplistic way to use Scoring UI is to use the input form. Input \u00b6 Scoring UI dynamically generates input form based on raw inputs defined in Showcase project. Figure 1 - Sample Input Form Form is equipped to handle input validation like integer ranges, enum types, etc, as defined in the Showcase project. Also description for each input field is available as a tool tip to the \"i\" icon at the end of every form line. In case user has marked few input fields as important, Scoring UI presents just the important fields. This is particularly useful for models which have large number of inputs. Limiting the form to just the most important fields makes it easy for anyone to try out the model. In case someone wishes to input all the fields, one can toggle to \"View All\" view from top right corner of the form. Output \u00b6 Once input form is submitted, Scoring UI calls the RESTful API /predict for the Showcase project to get back the prediction. Output from the REST endpoint is displayed on the right portion of Scoring UI. Figure 2 - Sample Output Output as defined under Showcase project is displayed under section \"Output\" in figure above. For classification models, class probabilities are also displayed as shown in Fig 2. To make output realistic and user friendly, we can define icons, images, gifs for enum outputs along with some custom messages. Details about how to configure these custom outputs can be found here Feedback \u00b6 Figure 3 - Sample Feedback Along with output variables, Scoring UI also displays Response Time taken by the model to make the prediction. This is a good indicator about how efficiently model as well as the infrastructure where model is deployed, is performing. Users or testers can also give feedback with a thumbs up or thumbs down, for how they feel about the output. This helps build up feedback analytics about model performance. More details about model monitoring and analytics can be found here","title":"Form based Scoring"},{"location":"showcase/scoremodels/scoringui/formbasedscoring/#input","text":"Scoring UI dynamically generates input form based on raw inputs defined in Showcase project. Figure 1 - Sample Input Form Form is equipped to handle input validation like integer ranges, enum types, etc, as defined in the Showcase project. Also description for each input field is available as a tool tip to the \"i\" icon at the end of every form line. In case user has marked few input fields as important, Scoring UI presents just the important fields. This is particularly useful for models which have large number of inputs. Limiting the form to just the most important fields makes it easy for anyone to try out the model. In case someone wishes to input all the fields, one can toggle to \"View All\" view from top right corner of the form.","title":"Input"},{"location":"showcase/scoremodels/scoringui/formbasedscoring/#output","text":"Once input form is submitted, Scoring UI calls the RESTful API /predict for the Showcase project to get back the prediction. Output from the REST endpoint is displayed on the right portion of Scoring UI. Figure 2 - Sample Output Output as defined under Showcase project is displayed under section \"Output\" in figure above. For classification models, class probabilities are also displayed as shown in Fig 2. To make output realistic and user friendly, we can define icons, images, gifs for enum outputs along with some custom messages. Details about how to configure these custom outputs can be found here","title":"Output"},{"location":"showcase/scoremodels/scoringui/formbasedscoring/#feedback","text":"Figure 3 - Sample Feedback Along with output variables, Scoring UI also displays Response Time taken by the model to make the prediction. This is a good indicator about how efficiently model as well as the infrastructure where model is deployed, is performing. Users or testers can also give feedback with a thumbs up or thumbs down, for how they feel about the output. This helps build up feedback analytics about model performance. More details about model monitoring and analytics can be found here","title":"Feedback"},{"location":"showcase/scoremodels/scoringui/introduction/","text":"Clouderizer Showcase offers an out of box, autogenerated Scoring UI for all deployed models. It is a modern web interface which allows anyone to feed in model inputs and get back scored output. It also allows users to give feedback about how they feel model is performing. This is an excellent tool for Testing and Validation of model from external teams Showcasing model to clients, beta customers, senior management Playing around with model Figure 1 - Sample Scoring UI Scoring UI is built on top of modern web interface technology. Its fully responsive and works well on both desktop and mobiles. Figure 2 - Scoring UI on mobile Scoring UI endpoint can be made public, such that it can be shared with people who don't have Clouderizer account. Base URL for project REST endpoint loads this application. Easiest way to access it would be to click on the URL on the bottom left screen of running Showcase project.","title":"Introduction"},{"location":"showcase/uploadmodels/configure/","text":"Once Showcase project is created Showcase project offers following configurations Note: This configuration does not apply for Notebook project type. Input \u00b6 Every ML model requires input, using which it generates output. Input is represented by first block in Showcase project workflow. For MOJO and PMML projects, Showcase parses the model and automatically extracts and pre-configures model inputs. For Python models, this needs to be specified explicitly. At any point of time, we can modify model inputs by pressing the Configure button inside input block. Input configuration allows us to specify various input items for the model. In case of MOJO and PMML, model input line items are auto derived from model file and cannot be added/removed. But we can alter certain properties of model input line items. Each input line item has following properties Variable - This is the name which model actually recognises. In case of H2O and PMML, this is burnt in model and is a read only field. This property is used in Showcase input validation. In case a scoring request comes with a missing input or a mis-spelt input, request is rejected with Validation error. Name - This is a user friendly name for the model input item. We can specify whatever we want. This is the name shown to end user on Scoring UI . Important - This allows user to specify if this particular input is important. If some fields are marked as important, Scoring UI automatically shows only important inputs on first load. Users can press View All to see all inputs. It is useful for models that have large number of inputs and we want to make it easy of people to test the model with important inputs using Scoring UI. Description - This is again used for Scoring UI. Users testing the model via Scoring UI, can see this description to know more about this input. Field Type - User can specify field to be one of the following types Text Multi Line Text Enum Integer Number This property is used in Showcase input validation. In case a socoring request comes with inputs not following the type specified, request is rejected with Validation error. This property is also used to design input form for Socring UI. Customize - This property is used to add more validation rules for inputs. Enum categories can be defined for Enum input types. For integer/number input types, we can specify the range within which input should fall. This property is also used for input validation and for designing input form for Scoring UI. Output \u00b6 Output from the model, is represented by the last block in Showcase project workflow. For MOJO and PMML projects, Showcase parses the model and automatically extracts and pre-configures model output. For Python models, this needs to be specified explicitly. At any point of time, we can modify model output schema by pressing the Configure button inside output block. Output configuration allows us to specify various output elements for the model. In case of MOJO and PMML, model output line items are auto derived from model file and cannot be added/removed. But we can alter certain properties of model output line items. Output line item properties are similar to Input line item properties described above. We have an additional column for Advanced Settings in output. Details about this setting is covered under customizations for Scoring UI .","title":"Configure"},{"location":"showcase/uploadmodels/configure/#input","text":"Every ML model requires input, using which it generates output. Input is represented by first block in Showcase project workflow. For MOJO and PMML projects, Showcase parses the model and automatically extracts and pre-configures model inputs. For Python models, this needs to be specified explicitly. At any point of time, we can modify model inputs by pressing the Configure button inside input block. Input configuration allows us to specify various input items for the model. In case of MOJO and PMML, model input line items are auto derived from model file and cannot be added/removed. But we can alter certain properties of model input line items. Each input line item has following properties Variable - This is the name which model actually recognises. In case of H2O and PMML, this is burnt in model and is a read only field. This property is used in Showcase input validation. In case a scoring request comes with a missing input or a mis-spelt input, request is rejected with Validation error. Name - This is a user friendly name for the model input item. We can specify whatever we want. This is the name shown to end user on Scoring UI . Important - This allows user to specify if this particular input is important. If some fields are marked as important, Scoring UI automatically shows only important inputs on first load. Users can press View All to see all inputs. It is useful for models that have large number of inputs and we want to make it easy of people to test the model with important inputs using Scoring UI. Description - This is again used for Scoring UI. Users testing the model via Scoring UI, can see this description to know more about this input. Field Type - User can specify field to be one of the following types Text Multi Line Text Enum Integer Number This property is used in Showcase input validation. In case a socoring request comes with inputs not following the type specified, request is rejected with Validation error. This property is also used to design input form for Socring UI. Customize - This property is used to add more validation rules for inputs. Enum categories can be defined for Enum input types. For integer/number input types, we can specify the range within which input should fall. This property is also used for input validation and for designing input form for Scoring UI.","title":"Input"},{"location":"showcase/uploadmodels/configure/#output","text":"Output from the model, is represented by the last block in Showcase project workflow. For MOJO and PMML projects, Showcase parses the model and automatically extracts and pre-configures model output. For Python models, this needs to be specified explicitly. At any point of time, we can modify model output schema by pressing the Configure button inside output block. Output configuration allows us to specify various output elements for the model. In case of MOJO and PMML, model output line items are auto derived from model file and cannot be added/removed. But we can alter certain properties of model output line items. Output line item properties are similar to Input line item properties described above. We have an additional column for Advanced Settings in output. Details about this setting is covered under customizations for Scoring UI .","title":"Output"},{"location":"showcase/uploadmodels/createproject/","text":"Clouderizer Projects are created using model files (ONNX or MOJO or PMML or pickle) or Jupyter Notebooks. Upload Jupyter Notebook \u00b6 It is assumed you have a perfectly running Jupyter Notebook with you. This notebook can be about anything from Data Exploration, Data visualization, Data analysis, Model Training, Prediction, Model Scoring. Once deployed, notebook serverless endpoint can be invoked to run on CPU or GPU any number of times with different input parameters. This allows to design and deploy robust, cost-effective and scalable MLOps stages. Clouderizer stores executed Notebook and any output files generated from Notebook execution as an artifact. Notebook deployment can be updated with newer version of notebooks as well. For CLI based instruction for creating a notebook project please refer here . Goto Showcase Tab Press New Project Give project a name and description and select Notebook as project type. Press Next Drag and Drop the notebook file from your computer. In case your project requires specific libraries, list them in a requirements.txt file and upload that as well. Press Finish This should trigger start your project creation. Once project is created, it is ready to be deployed to different kind of infrastructure. Please refer this for deployment instructions. Upload Model \u00b6 It is assumed you have successfully exported your ML/DL model to a model file and have it available with you on your computer.These become first version of models for the project. Once created, these projects can be deployed, scored and monitored. Later, when needed, models inside a project can be updated as well. To create a project Goto Showcase Tab Press New Project Give project a name and description and press Next Select the model type that you wish to upload and press Next . Supported model types are MOJO, PMML and Python pickle files. Drag and Drop the model file from your computer and press Finish This should trigger start your project creation. Behind scenes, Showcase will upload your model, parse the model to get input / output parameters (only for MOJO and PMML models). Once project gets created, it will offer you to have a look at the detected model input parameters. More information about configuring model input/output parameters can be found here","title":"Create Project"},{"location":"showcase/uploadmodels/createproject/#upload-jupyter-notebook","text":"It is assumed you have a perfectly running Jupyter Notebook with you. This notebook can be about anything from Data Exploration, Data visualization, Data analysis, Model Training, Prediction, Model Scoring. Once deployed, notebook serverless endpoint can be invoked to run on CPU or GPU any number of times with different input parameters. This allows to design and deploy robust, cost-effective and scalable MLOps stages. Clouderizer stores executed Notebook and any output files generated from Notebook execution as an artifact. Notebook deployment can be updated with newer version of notebooks as well. For CLI based instruction for creating a notebook project please refer here . Goto Showcase Tab Press New Project Give project a name and description and select Notebook as project type. Press Next Drag and Drop the notebook file from your computer. In case your project requires specific libraries, list them in a requirements.txt file and upload that as well. Press Finish This should trigger start your project creation. Once project is created, it is ready to be deployed to different kind of infrastructure. Please refer this for deployment instructions.","title":"Upload Jupyter Notebook"},{"location":"showcase/uploadmodels/createproject/#upload-model","text":"It is assumed you have successfully exported your ML/DL model to a model file and have it available with you on your computer.These become first version of models for the project. Once created, these projects can be deployed, scored and monitored. Later, when needed, models inside a project can be updated as well. To create a project Goto Showcase Tab Press New Project Give project a name and description and press Next Select the model type that you wish to upload and press Next . Supported model types are MOJO, PMML and Python pickle files. Drag and Drop the model file from your computer and press Finish This should trigger start your project creation. Behind scenes, Showcase will upload your model, parse the model to get input / output parameters (only for MOJO and PMML models). Once project gets created, it will offer you to have a look at the detected model input parameters. More information about configuring model input/output parameters can be found here","title":"Upload Model"},{"location":"showcase/uploadmodels/debuggingsetup/","text":"Clouderizer Showcase allows us to test our preprocessing, prediction and postprocessing code in the console itself. This testing requires a debugging env setup on your local machine. Pre-requisites Linux or Mac machines with Docker installed and running. Min. 4GB of RAM Lets go over the steps needed for this setup. Open your Showcase project and launch code editor for any of your pre-processing or prediction block. Figure 1 - Prediction code button First time, you see following message at the top of code editor window. Figure 2 - Setup instructions Copy the curl command by pressing the copy button at the end of text box. Figure 3 - Copy command Open a terminal on your machine and paste the command copied above and press Enter. Figure 4 - Run on terminal This should start the dev docker env on your machine. Once docker is up and running, switch back to Showcase console code editor view. It should detect the dev environment and show Connected to Kernel Figure 5 - Connected to kernel You can now test your code by pressing Run button. Output should appear on the right. In case you have added some dependency pip package to your pre-process or prediction code, and want to install the same package in your debugging env as well, press the install button on the right of pip packages text list. You can see the progress of package installation from the output view. Figure 6 - Install dependencies","title":"Debugging Setup"},{"location":"showcase/uploadmodels/postprocessing/","text":"Coming Soon \u00b6","title":"Postprocessing"},{"location":"showcase/uploadmodels/postprocessing/#coming-soon","text":"","title":"Coming Soon"},{"location":"showcase/uploadmodels/prediction/","text":"Once we create the project and configure its inputs and outputs, we are all set. PMML and MOJO projects are completely code-less. Which means we don't need to specify any code to deploy these models. Clouderizer's PMML and MOJO scoring engine takes care of that. In case of Python projects, users need to provide a prediction code snippet, which uses the python pickle file and performs prediction. Showcase takes this snippet and integrates it with the deployment pipeline, offering end to end robust, scalable, secure, manageable deployment. Figure 1 - Prediction code button To configure the prediction code, press Prediction Code button under Model block. This brings up console code editor. Code \u00b6 Figure 2 - Prediction code Code editor scaffolds a predict python function that needs to be filled up. Input to this function is a python object data , an array containing model inputs. Scaffoled code also includes lines which loads the model pickle file and offers it as a python object. We need to paste our prediction code inside the predict function. Output from this function needs to be another array matching schema for model output. We are free to include any other helper code outside the predict function like importing other libs, defining other functions and classes, declaring and initializing global variables, etc. Libs \u00b6 Figure 3 - Configure preprocess libs In case your pre-processing code requires other python libraries, you can add your dependency packages here. Go to the text field at the bottom of code editor, which says Add pip packages , type in your pip package name and press Enter. Add as many packages as you want. Make sure to press Save button to ensure these dependency packages list is saved. Test \u00b6 Figure 4 - Test preprocess code Clouderizer console allows us to test our code within the code editor itself. It requires us to get debugging setup. Please follow instructions here to set this up. Once your setup is proper, code editor shows Connected to Kernel message on top left. Once our code editor shows that it is connected to kernel, we can press Run button at bottom of the code editor to run the code. Make sure you fill the input data object with some dummy input data for testing. Output shall appear on the right under Output. Once you are satisfied with your pre-processing code, press Save and Upload button to save it. Tip Make sure to press Save button as you are modifying your code and dependency package list to ensure your changes are saved.","title":"Prediction"},{"location":"showcase/uploadmodels/prediction/#code","text":"Figure 2 - Prediction code Code editor scaffolds a predict python function that needs to be filled up. Input to this function is a python object data , an array containing model inputs. Scaffoled code also includes lines which loads the model pickle file and offers it as a python object. We need to paste our prediction code inside the predict function. Output from this function needs to be another array matching schema for model output. We are free to include any other helper code outside the predict function like importing other libs, defining other functions and classes, declaring and initializing global variables, etc.","title":"Code"},{"location":"showcase/uploadmodels/prediction/#libs","text":"Figure 3 - Configure preprocess libs In case your pre-processing code requires other python libraries, you can add your dependency packages here. Go to the text field at the bottom of code editor, which says Add pip packages , type in your pip package name and press Enter. Add as many packages as you want. Make sure to press Save button to ensure these dependency packages list is saved.","title":"Libs"},{"location":"showcase/uploadmodels/prediction/#test","text":"Figure 4 - Test preprocess code Clouderizer console allows us to test our code within the code editor itself. It requires us to get debugging setup. Please follow instructions here to set this up. Once your setup is proper, code editor shows Connected to Kernel message on top left. Once our code editor shows that it is connected to kernel, we can press Run button at bottom of the code editor to run the code. Make sure you fill the input data object with some dummy input data for testing. Output shall appear on the right under Output. Once you are satisfied with your pre-processing code, press Save and Upload button to save it. Tip Make sure to press Save button as you are modifying your code and dependency package list to ensure your changes are saved.","title":"Test"},{"location":"showcase/uploadmodels/preprocessing/","text":"We can enable pre-processing on user input before we feed it into the model for evaluation. Note: This configuration does not apply for Notebook project type. Figure 1 - Pre-processing workflow Preprocessing can be enabled by going into Showcase project Settings and enabling option Need Preprocessing Figure 2 - Enable pre-processing Once pre-processing is enabled, project pipleline workflow changes as per Figure 1 above. Two new blocks get added before Model Input Raw Input \u00b6 Figure 3 - Raw input configuration This is the main input to the project. Raw input is completely defined by user. We can press Configure on Raw input block to bring up its configuration. Here we can add / remove input line items and configure its Name, Description, Type, Enum types, ranges, etc. Preprocessing \u00b6 Figure 4 - Configure preprocess code We can specify pre-processing python code that transforms raw input into a form that can be consumed by our model. Press Preprocess Code button on Preprocessing block to open the code editor. Code \u00b6 Figure 5 - Configure preprocess code Code editor scaffolds a preprocess python function that needs to be filled up. Input to this function is a python object data , an array containing raw inputs. We need to paste our pre-processing code inside the predict function. Output from this function needs to be another array which will be input into the model. We are free to include any other helper code outside the preprocess function like importing other libs, defining other functions and classes, declaring and initializing global variables, etc. Libs \u00b6 Figure 6 - Configure preprocess libs In case your pre-processing code requires other python libraries, you can add them here. Go to the text field at the bottom of code editor, which says Add pip packages , type in your pip package name and press Enter. Add as many packages as you want. Make sure to press Save button to ensure these dependency packages list is saved. Test \u00b6 Figure 7 - Test preprocess code Clouderizer console allows us to test our code within the code editor itself. It requires us to get debugging setup. Please follow instructions here to set this up. Once your setup is proper, code editor shows Connected to Kernel message on top left. Once our code editor shows that it is connected to kernel, we can press Run button at bottom of the code editor to run the code. Make sure you fill the input data object with some dummy raw input data for testing. Output shall appear on the right under Output. Once you are satisfied with your pre-processing code, press Save and Upload button to save it. Tip Make sure to press Save button as you are modifying your code and dependency package list to ensure your changes are saved. Model Input \u00b6 Figure 8 - Model input Model input is what is fed to model for scoring. Output from preprocess code (what we return from preprocess function above) should be in the format of model input scheam. We can press Configure button on model input block to access its configuration. In case of PMML and MOJO projects, model input line items are fixed and read only. We can just modify its name, description and field types if needed. In case of Python projects, complete model input schema is configurable. We can add / remove input line items as per our project specifications. Model input schema is used in Showcase input validation. Which means if our pre-process code is not able to produce output as defined in model input config here, request fails input validation.","title":"Preprocessing"},{"location":"showcase/uploadmodels/preprocessing/#raw-input","text":"Figure 3 - Raw input configuration This is the main input to the project. Raw input is completely defined by user. We can press Configure on Raw input block to bring up its configuration. Here we can add / remove input line items and configure its Name, Description, Type, Enum types, ranges, etc.","title":"Raw Input"},{"location":"showcase/uploadmodels/preprocessing/#preprocessing","text":"Figure 4 - Configure preprocess code We can specify pre-processing python code that transforms raw input into a form that can be consumed by our model. Press Preprocess Code button on Preprocessing block to open the code editor.","title":"Preprocessing"},{"location":"showcase/uploadmodels/preprocessing/#code","text":"Figure 5 - Configure preprocess code Code editor scaffolds a preprocess python function that needs to be filled up. Input to this function is a python object data , an array containing raw inputs. We need to paste our pre-processing code inside the predict function. Output from this function needs to be another array which will be input into the model. We are free to include any other helper code outside the preprocess function like importing other libs, defining other functions and classes, declaring and initializing global variables, etc.","title":"Code"},{"location":"showcase/uploadmodels/preprocessing/#libs","text":"Figure 6 - Configure preprocess libs In case your pre-processing code requires other python libraries, you can add them here. Go to the text field at the bottom of code editor, which says Add pip packages , type in your pip package name and press Enter. Add as many packages as you want. Make sure to press Save button to ensure these dependency packages list is saved.","title":"Libs"},{"location":"showcase/uploadmodels/preprocessing/#test","text":"Figure 7 - Test preprocess code Clouderizer console allows us to test our code within the code editor itself. It requires us to get debugging setup. Please follow instructions here to set this up. Once your setup is proper, code editor shows Connected to Kernel message on top left. Once our code editor shows that it is connected to kernel, we can press Run button at bottom of the code editor to run the code. Make sure you fill the input data object with some dummy raw input data for testing. Output shall appear on the right under Output. Once you are satisfied with your pre-processing code, press Save and Upload button to save it. Tip Make sure to press Save button as you are modifying your code and dependency package list to ensure your changes are saved.","title":"Test"},{"location":"showcase/uploadmodels/preprocessing/#model-input","text":"Figure 8 - Model input Model input is what is fed to model for scoring. Output from preprocess code (what we return from preprocess function above) should be in the format of model input scheam. We can press Configure button on model input block to access its configuration. In case of PMML and MOJO projects, model input line items are fixed and read only. We can just modify its name, description and field types if needed. In case of Python projects, complete model input schema is configurable. We can add / remove input line items as per our project specifications. Model input schema is used in Showcase input validation. Which means if our pre-process code is not able to produce output as defined in model input config here, request fails input validation.","title":"Model Input"},{"location":"workspace/introduction/","text":"Clouderizer Workspace is a cloud IDE for Data Scientists. It allows us to setup, train and deploy AI/ML projects on any infrastructure (AWS, GCP, Local), without worrying about underlying DevOps. Optimized for Machine Learning Developers \u00b6 Built in project templates with ML tools like Tensorflow, Keras, Anaconda, Python, Torch. With few clicks you can select machine type, setup environment, upload your deep learning model, download data sets and kick start training, all automated, in one go. Run projects locally, on cloud or both \u00b6 Clouderizer Workspace projects can run locally on any Ubuntu / OS X / Windows machine or on any cloud machine. Projects running on local machine can be switched to run on cloud and vice versa. Code, dataset and output checkpoints are synced with Clouderizer Drive in real time. No matter where your project runs, you always resume from where you last stopped. Forget DevOps. Focus on Machine Learning. \u00b6 Clouderizer Workspace by default creates a docker environment with most popular libraries and frameworks, with GPU support, for Machine Learning. You can specify additional apt, brew, conda, pip, torch lua packages or custom shell scripts needed to setup your environment. Once configured, environment is setup automatically on any machine you run. Secure Terminal, Jupyter and Tensorboard \u00b6 Access your Clouderizer machine from anywhere using our secure private tunnel. SSH terminal, Jupyter Notebooks and Tensorboard are securely accessible from Clouderizer Web Console. User Management \u00b6 Create users and allocate cloud resources to team members. Monitor usage across organization. Share project templates and projects within organization. Ideal for use in schools, universities, training programs and software development teams.","title":"Introduction"},{"location":"workspace/introduction/#optimized-for-machine-learning-developers","text":"Built in project templates with ML tools like Tensorflow, Keras, Anaconda, Python, Torch. With few clicks you can select machine type, setup environment, upload your deep learning model, download data sets and kick start training, all automated, in one go.","title":"Optimized for Machine Learning Developers"},{"location":"workspace/introduction/#run-projects-locally-on-cloud-or-both","text":"Clouderizer Workspace projects can run locally on any Ubuntu / OS X / Windows machine or on any cloud machine. Projects running on local machine can be switched to run on cloud and vice versa. Code, dataset and output checkpoints are synced with Clouderizer Drive in real time. No matter where your project runs, you always resume from where you last stopped.","title":"Run projects locally, on cloud or both"},{"location":"workspace/introduction/#forget-devops-focus-on-machine-learning","text":"Clouderizer Workspace by default creates a docker environment with most popular libraries and frameworks, with GPU support, for Machine Learning. You can specify additional apt, brew, conda, pip, torch lua packages or custom shell scripts needed to setup your environment. Once configured, environment is setup automatically on any machine you run.","title":"Forget DevOps. Focus on Machine Learning."},{"location":"workspace/introduction/#secure-terminal-jupyter-and-tensorboard","text":"Access your Clouderizer machine from anywhere using our secure private tunnel. SSH terminal, Jupyter Notebooks and Tensorboard are securely accessible from Clouderizer Web Console.","title":"Secure Terminal, Jupyter and Tensorboard"},{"location":"workspace/introduction/#user-management","text":"Create users and allocate cloud resources to team members. Monitor usage across organization. Share project templates and projects within organization. Ideal for use in schools, universities, training programs and software development teams.","title":"User Management"},{"location":"workspace/data/accessing_data/","text":"Tip Write original","title":"Accessing data"},{"location":"workspace/data/project_folders/","text":"Clouderizer, by default, creates a folder named clouderizer under the user home directory on any machine it is run. This folder contains sub-folders for each individual project that is run on this machine. Inside each project folder are 3 important folders, data, code and output We shall now go through each of these folders one by one. Data \u00b6 Data is the directory which expects to house data involved in a project. This can be training/test/validation data or any other kind of static file that our project might need. By default, this path follows this schema ~/clouderizer/<ProjectName>/data/ Datasets specified in URL Datasets and Kaggle Datasets get downloaded to data folder on the machine where the project is run for first time. After first run, data folder is backed and synced to your Google Drive, so that any changes you make while your project is running is persisted and ready for next run. Code \u00b6 Code is the directory which houses our project files. These can be source code, scripts, practically all kinds of files that you have created/assembled to make our project work. By default, this path follows this schema ~/clouderizer/ /code/ CODE field allows us to specify any Git URL to initialise code folder whenever project runs on a machine. In case URL Git authentication, we can press Auth button inside the input box to provide those credentials. Code is downloaded from Git on first run. After first run, this folder is backed and synced to your Google Drive, so that any changes you make while your project is running is persisted and ready for next run. Out \u00b6 Out is the directory where output of our project should be saved. Output like model weights, intermediate check points should be saved in this directory. This directory also gets synced to Google Drive, allowing you to save your models during your experiments.","title":"Project folders"},{"location":"workspace/data/project_folders/#data","text":"Data is the directory which expects to house data involved in a project. This can be training/test/validation data or any other kind of static file that our project might need. By default, this path follows this schema ~/clouderizer/<ProjectName>/data/ Datasets specified in URL Datasets and Kaggle Datasets get downloaded to data folder on the machine where the project is run for first time. After first run, data folder is backed and synced to your Google Drive, so that any changes you make while your project is running is persisted and ready for next run.","title":"Data"},{"location":"workspace/data/project_folders/#code","text":"Code is the directory which houses our project files. These can be source code, scripts, practically all kinds of files that you have created/assembled to make our project work. By default, this path follows this schema ~/clouderizer/ /code/ CODE field allows us to specify any Git URL to initialise code folder whenever project runs on a machine. In case URL Git authentication, we can press Auth button inside the input box to provide those credentials. Code is downloaded from Git on first run. After first run, this folder is backed and synced to your Google Drive, so that any changes you make while your project is running is persisted and ready for next run.","title":"Code"},{"location":"workspace/data/project_folders/#out","text":"Out is the directory where output of our project should be saved. Output like model weights, intermediate check points should be saved in this directory. This directory also gets synced to Google Drive, allowing you to save your models during your experiments.","title":"Out"},{"location":"workspace/data/sync_to_googledrive/","text":"Clouderizer Drive \u00b6 Clouderizer Drive is a cloud storage which is used to back up project data/code/output in cloud. Users can enable this by linking their Google Drive with Clouderizer. For every project we create, a code/data/out directory is created in Clouderizer Drive automatically. Sync Up \u00b6 While a clouderizer project is running on a machine, its code, data and out folders are synced up to Clouderizer Drive every minute. This sync does not delete any files on Clouderizer Drive. It only copies new and modified files from local folders to Clouderizer. This Sync Up helps us to backup our work, any local changes done on code files, datasets, model weights and checkpoints. Sync Down \u00b6 While a project is running on a machine, its data folder is synced down from Clouderizer Drive every two minutes. This sync does not delete any files locally on the machine. It only copies new and modified files from Clouderizer Drive to local machine. This Sync Down helps us to transfer datasets (or any other kind of data) to our machine running the project. At any point of time we just need to upload our datasets to the data folder of our project on Google Drive. Within two minutes this data will be downloaded in data folder of our project on the machine where project is running. Project First start \u00b6 Whenever Clouderizer project is started for first time after creation, code and data folders are downloaded from sources specified in project settings (Git URL / Kaggle Dataset / URL Datasets). These downloaded files are then completely backed up to the project folder on Google Drive. Any changes made by user on these folders also get synced up to Google Drive. Project Subsequent starts \u00b6 Every time Clouderizer projects are run subsequently, code, data and out folders are downloaded from Google Drive (instead of original source) with your latest changes. Any changes made by user on these folders get synced up to Google Drive for persistence.","title":"Sync to googledrive"},{"location":"workspace/data/sync_to_googledrive/#clouderizer-drive","text":"Clouderizer Drive is a cloud storage which is used to back up project data/code/output in cloud. Users can enable this by linking their Google Drive with Clouderizer. For every project we create, a code/data/out directory is created in Clouderizer Drive automatically.","title":"Clouderizer Drive"},{"location":"workspace/data/sync_to_googledrive/#sync-up","text":"While a clouderizer project is running on a machine, its code, data and out folders are synced up to Clouderizer Drive every minute. This sync does not delete any files on Clouderizer Drive. It only copies new and modified files from local folders to Clouderizer. This Sync Up helps us to backup our work, any local changes done on code files, datasets, model weights and checkpoints.","title":"Sync Up"},{"location":"workspace/data/sync_to_googledrive/#sync-down","text":"While a project is running on a machine, its data folder is synced down from Clouderizer Drive every two minutes. This sync does not delete any files locally on the machine. It only copies new and modified files from Clouderizer Drive to local machine. This Sync Down helps us to transfer datasets (or any other kind of data) to our machine running the project. At any point of time we just need to upload our datasets to the data folder of our project on Google Drive. Within two minutes this data will be downloaded in data folder of our project on the machine where project is running.","title":"Sync Down"},{"location":"workspace/data/sync_to_googledrive/#project-first-start","text":"Whenever Clouderizer project is started for first time after creation, code and data folders are downloaded from sources specified in project settings (Git URL / Kaggle Dataset / URL Datasets). These downloaded files are then completely backed up to the project folder on Google Drive. Any changes made by user on these folders also get synced up to Google Drive.","title":"Project First start"},{"location":"workspace/data/sync_to_googledrive/#project-subsequent-starts","text":"Every time Clouderizer projects are run subsequently, code, data and out folders are downloaded from Google Drive (instead of original source) with your latest changes. Any changes made by user on these folders get synced up to Google Drive for persistence.","title":"Project Subsequent starts"},{"location":"workspace/projects/overview/","text":"Clouderizer offers a platform to address various road blocks & challenges involved in developing deep learning models, like Cost effective Cloud infrastructure, Hassle free Environment setup, Secure and Easy Remote access Cloud based workspace for easy modifications and iterations Clouderizer tries to bring together all these aspects of Deep Learning development under one umbrella as Clouderizer Projects.","title":"Overview"},{"location":"workspace/projects/code/git/","text":"You can configure to automatically download code from any Git repositary on project start. Following are the steps Login to your Clouderizer console. Create a new project or modify an existing project and go to WORKSPACE tab. Enter full http URL of the git repositary. In case git repositary is behind Authentication, you can specify the credentials by pressing Auth button toward the right end of Code input box. Once you have saved the Clouderizer project with required git url, you can start this project on any machine of your choice. On start, Clouderizer will automatically download the specified repositary and save it inside code folder of clouderizer project. These code files are backed up in Google Drive in real time. Any changes made to these files, while you work on the project, are also automatically saved to Google Drive (with a latency of upto 1 minute).","title":"Git"},{"location":"workspace/projects/code/url/","text":"You can configure to automatically download code zip archive from any URL on project start. Following are the steps Login to your Clouderizer console. Create a new project or modify an existing project and go to WORKSPACE tab. Enter full http URL of the code zip archive. In case http url is behind Basic Authentication, you can specify the credentials by pressing Auth button toward the right end of Code input box. Once you have saved the Clouderizer project with required code zip url, you can start this project on any machine of your choice. On start, Clouderizer will automatically download the specified code zip archive and save it inside code folder of clouderizer project. Clouderizer will automatically extract the .zip or .tar archive. These code files are backed up in Google Drive in real time. Any changes made to these files are also automatically saved to Google Drive (with a latency of upto 1 minute).","title":"Url"},{"location":"workspace/projects/dataset/kaggle/","text":"Clouderizer has integration with official Kaggle API. In order to use this integration, following one time activity needs to be performed. Login to your Kaggle account and go to My Account -> API and Create New API Token. This will download your API token file, kaggle.json, on your machine. Now login to your Clouderizer console and go to Settings -> Cloud Setting -> Kaggle Credentials. Choose kaggle.json file downloaded in step 1 and press Upload Kaggle Credentials. Note: These credentials are stored in our secure database and are only pushed and used on machines you run your Clouderizer projects on. At any point of time, you can revoke/expire these credentials by logging into your Kaggle Account and going to My Account -> API -> Expire API Token. Once above steps are done, every time you create / edit a project, a new option should appear under WORKSPACE tab, Kaggle Datasets. Here you can specify any Kaggle competition or dataset name, which you wish to automatically download once project starts. You can get this name from Data tab of any Kaggle competition page e.g. Note that you can specify more than one Kaggle dataset in a single Clouderizer project. Just type or paste the name of dataset and press enter. This should allow you to add more datasets Make sure you have accepted the Rules for the competition from Rules tab. Failing to do so, will prevent Clouderizer from downloading the dataset. Once you have saved the Clouderizer project with required Kaggle competition/dataset names, you can start this project on any machine of your choice. On start, Clouderizer will automatically download the specified dataset from Kaggle site and save them on data folder of clouderizer project. Inside data folder, we create two sub folders competitions and datasets. These folders further contain subfolders for various competitions and datasets specified in project setup. These datasets are backed up in Google Drive as a zip archive under your project folder. Any changes made to the dataset are also saved to Google Drive. This is very useful in automating the dataset download especially on instances like Google Colab, where transferring data and Kaggle credential files is not straight forward.","title":"Kaggle"},{"location":"workspace/projects/dataset/url/","text":"You can configure to automatically download datasets from any URL on project start. Following are the steps Login to your Clouderizer console. Create a new project or modify an existing project and go to WORKSPACE tab. Here you can specify full http URL of the dataset and press enter to add it to dataset list. You can specify more than one dataset if needed. Once you have saved the Clouderizer project with required dataset url, you can start this project on any machine of your choice. On start, Clouderizer will automatically download the specified dataset from Kaggle site and save them on data folder of clouderizer project. In case of zip or tar dataset, Clouderizer will automatically extract it and place it inside /data folder of the project. These datasets are backed up in Google Drive as a zip archive under your project folder. Any changes made to the dataset are also saved to Google Drive.","title":"Url"},{"location":"workspace/projects/dependency/apt/","text":"Clouderizer, out of box, runs all its projects in a ML optimised docker container, pre-built with commonly used ML frameworks like pytorch, tensorflow, lua torch, etc. (except in case of Google Colab, which runs in a pre-built docker container by Google). In case our pre-cooked docker image does not contain some dependency framework needed by your project, you can easily specify those packages in your project properties. SETUP tab in Clouderizer project allows us to specify packages that we would like to get installed before our project runs on a machine. We can specify 2 kinds of packages here Debian APT (Linux) Python PIP packages To specify a package in any of these categories, we just need to bring respective input box in focus and type the name of the package and press enter to add it to list. We can specify multiple packages. Here is an example Whenever Clouderizer project starts on a machine, it checks if the respective packages is already installed or not. If not, it will try to silently install it. This is a very useful feature to automate your project setup, irrespective of where you run them. Environments like Google Colab or AWS Spot instance or GCP Pre-emptible VMs where we generally have to setup new machines very often, such automation can save lots of manual effort.","title":"Apt"},{"location":"workspace/projects/dependency/pip/","text":"Clouderizer, out of box, runs all its projects in a ML optimised docker container, pre-built with commonly used ML frameworks like pytorch, tensorflow, lua torch, etc. (except in case of Google Colab, which runs in a pre-built docker container by Google). In case our pre-cooked docker image does not contain some dependency framework needed by your project, you can easily specify those packages in your project properties. SETUP tab in Clouderizer project allows us to specify packages that we would like to get installed before our project runs on a machine. We can specify 2 kinds of packages here Debian APT (Linux) Python PIP packages To specify a package in any of these categories, we just need to bring respective input box in focus and type the name of the package and press enter to add it to list. We can specify multiple packages. Here is an example Whenever Clouderizer project starts on a machine, it checks if the respective packages is already installed or not. If not, it will try to silently install it. This is a very useful feature to automate your project setup, irrespective of where you run them. Environments like Google Colab or AWS Spot instance or GCP Pre-emptible VMs where we generally have to setup new machines very often, such automation can save lots of manual effort.","title":"Pip"},{"location":"workspace/projects/dependency/requirements.txt/","text":"Tip Future Feature","title":"Requirements.txt"},{"location":"workspace/projects/scripts/setup/","text":"Very often ML projects require pre-define custom setup. Clouderizer offers a way to specify these custom setups as bash script in your project properties itself, so that these steps need not be repeated every time you start your project. Under WORKSPACE tab, we can find Setup and Startup scripts. Setup Script Script specified under Setup is executed very early in your project startup. This is before your code and data is downloaded in the docker container. This section is useful for scripting setups like installing complex dependencies (like Anaconda, which cannot be specified as simple PIP or APT package).","title":"Setup"},{"location":"workspace/projects/scripts/startup/","text":"Startup Script Script specified under Startup is executed once project is almost setup and ready. This is after your code, data and output folders are fully downloaded and synced with your Google Drive. This section is useful for auto kickstarting our training or data regularisation activities. This script can accomodate long running tasks without any issues. Startup Scripts are run in an independent tmux session, which means, it is possible to open and view this session anytime and evaluate its progress or troubleshoot any errors. Just open a remote terminal from the running project and type the following command to connect to startup tmux session tmux a -t startup","title":"Startup"},{"location":"workspace/running_projects/aws/","text":"Tip Borrow from https://help.clouderizer.com/running-on-cloud/aws/running-on-aws-project-setup","title":"Aws"},{"location":"workspace/running_projects/colab/","text":"Google Colab offers free, easy access to Tesla K80 GPUs to anyone with a Google account. We can run our projects on Google Colab instances with couple of manual steps, allowing us to automate our dependency setup, source and datasets download and remote Terminal, Jupyter, Tensorboard, Serving access. Below are the steps for this Login to your Clouderizer console and create new project or edit an existing project. You can also clone a new project from one of our many cool Community Templates like Fast.ai, Tensorflow Object Detection, Kaggle Competition, etc Give project a name and press Next and fill up details on Setup and Workspace tab as per your project requirements. Finish the wizard. You will now see the project in your project list. Press Start button for this project. Select Colab from the list of platforms. Press Launch Now. This will open a Colab notebook with a single cell. This cell contains bootstrap code for your project. Run the above cell. This should trigger automated Clouderizer project deployment on this instance. You can now switch back to Clouderizer console and see the progress of your project startup from here. Once project status changes to Running, all your project dependencies, source code, datasets and custom startup scripts will be setup and you can start working.","title":"Colab"},{"location":"workspace/running_projects/gcp/","text":"Now again you can press Start on your project and select Google Cloud Platform. You will now be presented with various GCP machine options to run your project on. You can press Start button for the machine option of your choice. When running for the first time, you might see an error like this This is due to GCP Compute Engine APIs not enabled yet for Clouderizer. This error message will give you a URL to enable this API. Copy and visit this URL and follow on-screen instructions to enable this API. This might take few minutes if you have signed up for GCP just now. Once Compute Engine APIs are enabled, you can come back to Clouderizer console and try starting the project again. It should now trigger automated process of creating a new VM instance in GCP, setting up your project environment, downloading your code and datasets. You can track the progress of your machine setup from Clouderizer console. Once your project setup is ready, its status should change to Running. You can now use JupyterLab and remote terminal button to work on your project. Important Note Some users, who have signed up for GCP recently, can see this error on starting their projects \u201cQuota \u2018GPUS_ALL_REGIONS\u2019 exceeded. Limit: 0.0 globally.\u201d Please go to Compute Engine -> Quotas -> Iam and admin (hyperlink) and search for the following quota GPUs (all region). In case limit for this is 0, you need to request Google to increase this to 1 or whatever value you need.","title":"Gcp"},{"location":"workspace/running_projects/kaggle/","text":"Tip Borrow from https://towardsdatascience.com/running-fast-ai-course-notebooks-on-kaggle-kernel-5b42e54a4a79","title":"Kaggle"},{"location":"workspace/running_projects/mac/","text":"","title":"Mac"},{"location":"workspace/running_projects/ubuntu/","text":"","title":"Ubuntu"}]}