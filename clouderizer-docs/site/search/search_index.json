{
    "docs": [
        {
            "location": "/",
            "text": "Clouderizer\n is an end to end MLOps platform for businesses.\n\n\nShowcase\n\u00b6\n\n\nBuilding a machine learning model to solve a real life business problem like Product Recommendation, Predicting Sales or Inventory, is only a small part of overall puzzle. Once built, model needs to be tested, deployed, integrated with business processes and systems, monitored, re-trained and re-deployed. Clouderizer Showcase helps to stitch together these stages in ML model lifecycle and deploy an end to end pipeline which allows enterprises to take their models from RnD & PoC stage to production and create value for their businesses.",
            "title": "Overview"
        },
        {
            "location": "/#showcase",
            "text": "Building a machine learning model to solve a real life business problem like Product Recommendation, Predicting Sales or Inventory, is only a small part of overall puzzle. Once built, model needs to be tested, deployed, integrated with business processes and systems, monitored, re-trained and re-deployed. Clouderizer Showcase helps to stitch together these stages in ML model lifecycle and deploy an end to end pipeline which allows enterprises to take their models from RnD & PoC stage to production and create value for their businesses.",
            "title": "Showcase"
        },
        {
            "location": "/showcase/introduction/",
            "text": "Journey of a ML model from RnD stage (with a bunch of Data Scientists) to a fully managed production stage involves going through mulitple stages and touching multiple teams. Some of these stages are\n\n\n\n\nDeploying the model\n\n\nTesting the model (with new real life data, validation by other teams)\n\n\nIntegrating the model with existing business application\n\n\nMonitoring and analysing the model performance with Ground Truth feedback\n\n\nRe-Training the model (with new data), once performance drops\n\n\nRe-deploying the new model version\n\n\nRepeat from Step 4\n\n\n\n\nClouderizer Showcase helps teams through each of these stages and setup and deploy an automated pipeline for ML models. Showcase supports deploying and managing the following\n\n\nParameterized Jupyter Notebooks\n\u00b6\n\n\nClouderizer allows you to publish your local Jupyter Notebooks to cloud as Serverless Functions. Once deployed these serverless functions can be invoked using a simple curl command. Moreover, Clouderizer supports parameterized Notebooks using \nPapermill\n. We can tag any cell in our notebook as \nparameter\n before deploying. Then while invoking the serverless function, we can pass any variable value in the http request as parameters.\n\n\nAs of now only Python kernel are supported in Jupyter Notebooks. R and Julia based kernel support is in works. In case you wish early access to this, please reach out to us at \ninfo@clouderizer.com\n\n\nH2O MOJO models\n\u00b6\n\n\nH2O.ai\n helps businesses to create machine learning models to extract insights from their data, without having in-house expertise in designing and tuning such models. It is one of the most popular open source AutoML platforms helping Citizen Data scientists import their business data and easily create highly effective machine learning models from them. H2O.ai AutoML libraries can be used in Python or R. H2O also offers an advanced AutoML console called Driverless AI. \nH2O.ai models can be exported in a Java executable format called MOJO. \n\n\nYou can upload these MOJO models into Clouderizer Showcase and deploy them. Clouderizer Showcase has deep integration with H2O MOJO scoring engine, such that, it can parse and score the models without requiring any custom code for pre-processing or prediction.\n\n\nHere\n is an example illustrating MOJO model deployment. \n\n\nPMML models\n\u00b6\n\n\nPMML\n\nPMML stands for \"Predictive Model Markup Language\". It is the de facto standard to represent predictive solutions. A PMML file may contain a myriad of data transformations (pre- and post-processing) as well as one or more predictive models.\nAnalytics platforms like Alteryx, RapidMiner, SaS, Dataiku have a direct way of exporting trained models in PMML. Models built using open source Python libs like sklearn, xgboost, lightGBM, etc, can also be exported to PMML using libraries like \nnyoka\n.\n\n\nYou can upload these PMML models into Clouderizer Showcase and deploy them. Clouderizer Showcase has deep integration with PMML scoring engine, such that, it can parse and score the models without requiring any custom code for pre-processing or prediction.\n\n\nHere\n is an example illustrating PMML model deployment.\n\n\nPython Pickle objects\n\u00b6\n\n\nOne of the most common way of saving a python model is using Python object serialization or pickling. These models are stored on disk as a pickle file. Pickle files can be loaded back from disk to recreate the python objects or the ML model. This python object serialization allows us to save models created using common python ML and DL frameworks like Tensorflow, PyTorch, Scikit-learn, etc.\n\n\nYou can upload these pickle file in Clouderizer Showcase and deploy them. While deploying python pickle models, we need to specify a \nprediction\n code snippet, which loads the model and does the prediction. You might also need to specify a \npre-processing\n code snippet as well, depending upon your project.\n\n\nHere\n is an example illustrating python pickle model deployment.\n\n\nONNX models (coming soon)\n\u00b6\n\n\nONNX\n is an open format built to represent machine learning models. While PMML standard tries to come up with a generic standard for Machine Learning models, ONNX aims to do the same for both Machine Learning as well as Deep Learning models as well.",
            "title": "Introduction"
        },
        {
            "location": "/showcase/introduction/#parameterized-jupyter-notebooks",
            "text": "Clouderizer allows you to publish your local Jupyter Notebooks to cloud as Serverless Functions. Once deployed these serverless functions can be invoked using a simple curl command. Moreover, Clouderizer supports parameterized Notebooks using  Papermill . We can tag any cell in our notebook as  parameter  before deploying. Then while invoking the serverless function, we can pass any variable value in the http request as parameters.  As of now only Python kernel are supported in Jupyter Notebooks. R and Julia based kernel support is in works. In case you wish early access to this, please reach out to us at  info@clouderizer.com",
            "title": "Parameterized Jupyter Notebooks"
        },
        {
            "location": "/showcase/introduction/#h2o-mojo-models",
            "text": "H2O.ai  helps businesses to create machine learning models to extract insights from their data, without having in-house expertise in designing and tuning such models. It is one of the most popular open source AutoML platforms helping Citizen Data scientists import their business data and easily create highly effective machine learning models from them. H2O.ai AutoML libraries can be used in Python or R. H2O also offers an advanced AutoML console called Driverless AI. \nH2O.ai models can be exported in a Java executable format called MOJO.   You can upload these MOJO models into Clouderizer Showcase and deploy them. Clouderizer Showcase has deep integration with H2O MOJO scoring engine, such that, it can parse and score the models without requiring any custom code for pre-processing or prediction.  Here  is an example illustrating MOJO model deployment.",
            "title": "H2O MOJO models"
        },
        {
            "location": "/showcase/introduction/#pmml-models",
            "text": "PMML \nPMML stands for \"Predictive Model Markup Language\". It is the de facto standard to represent predictive solutions. A PMML file may contain a myriad of data transformations (pre- and post-processing) as well as one or more predictive models.\nAnalytics platforms like Alteryx, RapidMiner, SaS, Dataiku have a direct way of exporting trained models in PMML. Models built using open source Python libs like sklearn, xgboost, lightGBM, etc, can also be exported to PMML using libraries like  nyoka .  You can upload these PMML models into Clouderizer Showcase and deploy them. Clouderizer Showcase has deep integration with PMML scoring engine, such that, it can parse and score the models without requiring any custom code for pre-processing or prediction.  Here  is an example illustrating PMML model deployment.",
            "title": "PMML models"
        },
        {
            "location": "/showcase/introduction/#python-pickle-objects",
            "text": "One of the most common way of saving a python model is using Python object serialization or pickling. These models are stored on disk as a pickle file. Pickle files can be loaded back from disk to recreate the python objects or the ML model. This python object serialization allows us to save models created using common python ML and DL frameworks like Tensorflow, PyTorch, Scikit-learn, etc.  You can upload these pickle file in Clouderizer Showcase and deploy them. While deploying python pickle models, we need to specify a  prediction  code snippet, which loads the model and does the prediction. You might also need to specify a  pre-processing  code snippet as well, depending upon your project.  Here  is an example illustrating python pickle model deployment.",
            "title": "Python Pickle objects"
        },
        {
            "location": "/showcase/introduction/#onnx-models-coming-soon",
            "text": "ONNX  is an open format built to represent machine learning models. While PMML standard tries to come up with a generic standard for Machine Learning models, ONNX aims to do the same for both Machine Learning as well as Deep Learning models as well.",
            "title": "ONNX models (coming soon)"
        },
        {
            "location": "/showcase/quickstart/",
            "text": "Clouderizer CLI\n\u00b6\n\n\nPre-requisites\n\u00b6\n\n\n\n\nSystem running MacOS / Ubuntu / Windows (with WSL)\n\n\nbash / sh / zsh terminal\n\n\nPython 3+\n\n\nPip for python3\n\n\n\n\nSteps\n\u00b6\n\n\n\n\nInstall Clouderizer CLI by running the following command in the terminal\n\npip install clouderizer\n\n\n\nLogin into your Clouderizer account using following command and following on-screen instructions\n\ncldz login\n\n\n\nNow cd into the directory where your notebook (say awesome-notebook.ipynb) resides. In case your notebook needs some specific libraries, put them in requirements.txt. Run the following command to deploy it on Clouderizer as Serverless function. Use \u2013infra flag to deploy it as a GPU function.\n\ncldz deploy awesome-notebook.ipynb requirements.txt --infra GPU\n\n\n\nThis command should push your notebook to cloud as serverless function and give you an http endpoint for the notebook. Something like this\n\n\n\n\nNOTEBOOK AYSNC URL: \nhttps://serverless.clouderizer.com/async-function/awesome-notebook-func\n\n\n\n\nCongratulations!! Your notebook is now transformed into a serverless function.\nYou can invoke your notebook using curl command\n\ncurl -i -X POST -F param=XYZ https://serverless.clouderizer.com/async-function/awesome-notebook-func \n\n\n\n\n\nWeb Console\n\u00b6\n\n\nPre-requisites\n\u00b6\n\n\n\n\nSystem with a modern browser like Chrome / FireFox / Safari / Edge\n\n\ncurl / postman / or any other http client\n\n\n\n\nSteps\n\u00b6\n\n\n\n\n\n\nSign up for a Clouderizer account from here and login into the web console from here.\n\n\n\n\n\n\nClick on New Project to create your project and give it a name. Select type as Notebook and press Next.\n\n\n\n\n\n\nBrowse your notebook file (awesome-notebook.ipynb). In case your notebook needs some specific libraries, upload your requirements.txt file as well. Press Finish.\n\n\n\n\n\n\nThis will upload your notebook and create the project.\n\n\n\n\n\n\nPress Deploy from top right corner of the screen. Select GPU from the infra type to deploy your notebook as a GPU function.\n\n\n\n\n\n\nThis should deploy your notebook as a serverless GPU function and you should see an http endpoint for your function in the status box in bottom left.\n\n\n\n\n\n\nCongratulations!! Your notebook is now transformed into a serverless function. You can invoke your notebook using curl command\n\ncurl -i -X POST -F param=XYZ https://serverless.clouderizer.com/function/async/awesome-notebook-func",
            "title": "Quick Start"
        },
        {
            "location": "/showcase/quickstart/#clouderizer-cli",
            "text": "",
            "title": "Clouderizer CLI"
        },
        {
            "location": "/showcase/quickstart/#pre-requisites",
            "text": "System running MacOS / Ubuntu / Windows (with WSL)  bash / sh / zsh terminal  Python 3+  Pip for python3",
            "title": "Pre-requisites"
        },
        {
            "location": "/showcase/quickstart/#steps",
            "text": "Install Clouderizer CLI by running the following command in the terminal pip install clouderizer  Login into your Clouderizer account using following command and following on-screen instructions cldz login  Now cd into the directory where your notebook (say awesome-notebook.ipynb) resides. In case your notebook needs some specific libraries, put them in requirements.txt. Run the following command to deploy it on Clouderizer as Serverless function. Use \u2013infra flag to deploy it as a GPU function. cldz deploy awesome-notebook.ipynb requirements.txt --infra GPU  This command should push your notebook to cloud as serverless function and give you an http endpoint for the notebook. Something like this   NOTEBOOK AYSNC URL:  https://serverless.clouderizer.com/async-function/awesome-notebook-func   Congratulations!! Your notebook is now transformed into a serverless function.\nYou can invoke your notebook using curl command curl -i -X POST -F param=XYZ https://serverless.clouderizer.com/async-function/awesome-notebook-func",
            "title": "Steps"
        },
        {
            "location": "/showcase/quickstart/#web-console",
            "text": "",
            "title": "Web Console"
        },
        {
            "location": "/showcase/quickstart/#pre-requisites_1",
            "text": "System with a modern browser like Chrome / FireFox / Safari / Edge  curl / postman / or any other http client",
            "title": "Pre-requisites"
        },
        {
            "location": "/showcase/quickstart/#steps_1",
            "text": "Sign up for a Clouderizer account from here and login into the web console from here.    Click on New Project to create your project and give it a name. Select type as Notebook and press Next.    Browse your notebook file (awesome-notebook.ipynb). In case your notebook needs some specific libraries, upload your requirements.txt file as well. Press Finish.    This will upload your notebook and create the project.    Press Deploy from top right corner of the screen. Select GPU from the infra type to deploy your notebook as a GPU function.    This should deploy your notebook as a serverless GPU function and you should see an http endpoint for your function in the status box in bottom left.    Congratulations!! Your notebook is now transformed into a serverless function. You can invoke your notebook using curl command curl -i -X POST -F param=XYZ https://serverless.clouderizer.com/function/async/awesome-notebook-func",
            "title": "Steps"
        },
        {
            "location": "/showcase/uploadmodels/createproject/",
            "text": "Clouderizer Projects are created using model files (ONNX or MOJO or PMML or pickle) or Jupyter Notebooks. \n\n\nUpload Jupyter Notebook\n\u00b6\n\n\nIt is assumed you have a perfectly running Jupyter Notebook with you. This notebook can be about anything from Data Exploration, Data visualization, Data analysis, Model Training, Prediction, Model Scoring. Once deployed, notebook serverless endpoint can be invoked to run on CPU or GPU any number of times with different input parameters. This allows to design and deploy robust, cost-effective and scalable MLOps stages. Clouderizer stores executed Notebook and any output files generated from Notebook execution as an artifact. Notebook deployment can be updated with newer version of notebooks as well.\n\n\nFor CLI based instruction for creating a notebook project please refer \nhere\n.\n\n\n\n\n\n\nGoto \nShowcase Tab\n\n\n\n\n\n\n\n\nPress \nNew Project\n\n\n\n\n\n\n\n\nGive project a name and description and select \nNotebook\n as project type. Press \nNext\n\n\n\n\n\n\nDrag and Drop the notebook file from your computer. In case your project requires specific libraries, list them in a requirements.txt file and upload that as well. Press \nFinish\n\n\n\n\n\n\nThis should trigger start your project creation. Once project is created, it is ready to be deployed to different kind of infrastructure. Please refer \nthis\n for deployment instructions.\n\n\nUpload Model\n\u00b6\n\n\nIt is assumed you have successfully exported your ML/DL model to a model file and have it available with you on your computer.These become first version of models for the project. Once created, these projects can be deployed, scored and monitored. Later, when needed, models inside a project can be updated as well.\n\n\nTo create a project\n\n\n\n\n\n\nGoto \nShowcase Tab\n\n\n\n\n\n\n\n\nPress \nNew Project\n\n\n\n\n\n\n\n\nGive project a name and description and press \nNext\n\n\n\n\n\n\n\n\nSelect the model type that you wish to upload and press \nNext\n. Supported model types are MOJO, PMML and Python pickle files.\n\n\n\n\n\n\n\n\nDrag and Drop the model file from your computer and press \nFinish\n\n\n\n\n\n\n\n\nThis should trigger start your project creation. Behind scenes, Showcase will upload your model, parse the model to get input / output parameters (only for MOJO and PMML models). Once project gets created, it will offer you to have a look at the detected model input parameters. More information about configuring model input/output parameters can be found \nhere",
            "title": "Create Project"
        },
        {
            "location": "/showcase/uploadmodels/createproject/#upload-jupyter-notebook",
            "text": "It is assumed you have a perfectly running Jupyter Notebook with you. This notebook can be about anything from Data Exploration, Data visualization, Data analysis, Model Training, Prediction, Model Scoring. Once deployed, notebook serverless endpoint can be invoked to run on CPU or GPU any number of times with different input parameters. This allows to design and deploy robust, cost-effective and scalable MLOps stages. Clouderizer stores executed Notebook and any output files generated from Notebook execution as an artifact. Notebook deployment can be updated with newer version of notebooks as well.  For CLI based instruction for creating a notebook project please refer  here .    Goto  Showcase Tab     Press  New Project     Give project a name and description and select  Notebook  as project type. Press  Next    Drag and Drop the notebook file from your computer. In case your project requires specific libraries, list them in a requirements.txt file and upload that as well. Press  Finish    This should trigger start your project creation. Once project is created, it is ready to be deployed to different kind of infrastructure. Please refer  this  for deployment instructions.",
            "title": "Upload Jupyter Notebook"
        },
        {
            "location": "/showcase/uploadmodels/createproject/#upload-model",
            "text": "It is assumed you have successfully exported your ML/DL model to a model file and have it available with you on your computer.These become first version of models for the project. Once created, these projects can be deployed, scored and monitored. Later, when needed, models inside a project can be updated as well.  To create a project    Goto  Showcase Tab     Press  New Project     Give project a name and description and press  Next     Select the model type that you wish to upload and press  Next . Supported model types are MOJO, PMML and Python pickle files.     Drag and Drop the model file from your computer and press  Finish     This should trigger start your project creation. Behind scenes, Showcase will upload your model, parse the model to get input / output parameters (only for MOJO and PMML models). Once project gets created, it will offer you to have a look at the detected model input parameters. More information about configuring model input/output parameters can be found  here",
            "title": "Upload Model"
        },
        {
            "location": "/showcase/uploadmodels/configure/",
            "text": "Once Showcase project is created Showcase project offers following configurations\n\n\nNote\n:\n \nThis\n \nconfiguration\n \ndoes\n \nnot\n \napply\n \nfor\n \nNotebook\n \nproject\n \ntype\n.\n\n\n\n\n\nInput\n\u00b6\n\n\n\nEvery ML model requires input, using which it generates output. \nInput\n is represented by first block in Showcase project workflow. For MOJO and PMML projects, Showcase parses the model and automatically extracts and pre-configures model inputs. For Python models, this needs to be specified explicitly. At any point of time, we can modify model inputs by pressing the \nConfigure\n button inside input block.\n\n\n\n\nInput configuration allows us to specify various input items for the model. In case of MOJO and PMML, model input line items are auto derived from model file and cannot be added/removed. But we can alter certain properties of model input line items. Each input line item has following properties\n\n\n\n\nVariable\n - This is the name which model actually recognises. In case of H2O and PMML, this is burnt in model and is a read only field. This property is used in Showcase input validation. In case a scoring request comes with a missing input or a mis-spelt input, request is rejected with Validation error.\n\n\nName\n - This is a user friendly name for the model input item. We can specify whatever we want. This is the name shown to end user on \nScoring UI\n.\n\n\nImportant\n - This allows user to specify if this particular input is important. If some fields are marked as important, Scoring UI automatically shows only important inputs on first load. Users can press \nView All\n to see all inputs. It is useful for models that have large number of inputs and we want to make it easy of people to test the model with important inputs using Scoring UI.\n\n\nDescription\n - This is again used for Scoring UI. Users testing the model via Scoring UI, can see this description to know more about this input.\n\n\n\n\nField Type\n - User can specify field to be one of the following types\n\n\n\n\nText\n\n\nMulti Line Text\n\n\nEnum\n\n\nInteger\n\n\nNumber\n\n\n\n\nThis property is used in Showcase input validation. In case a socoring request comes with inputs not following the type specified, request is rejected with Validation error. This property is also used to design input form for Socring UI.\n\n\n\n\n\n\nCustomize\n - This property is used to add more validation rules for inputs. Enum categories can be defined for Enum input types. For integer/number input types, we can specify the range within which input should fall. This property is also used for input validation and for designing input form for Scoring UI.\n\n\n\n\n\n\nOutput\n\u00b6\n\n\n\n\nOutput\n from the model, is represented by the last block in Showcase project workflow. For MOJO and PMML projects, Showcase parses the model and automatically extracts and pre-configures model output. For Python models, this needs to be specified explicitly. At any point of time, we can modify model output schema by pressing the \nConfigure\n button inside output block.\n\n\n\n\nOutput configuration allows us to specify various output elements for the model. In case of MOJO and PMML, model output line items are auto derived from model file and cannot be added/removed. But we can alter certain properties of model output line items. Output line item properties are similar to Input line item properties described above. We have an additional column for \nAdvanced Settings\n in output. Details about this setting is covered under \ncustomizations for Scoring UI\n.",
            "title": "Configure"
        },
        {
            "location": "/showcase/uploadmodels/configure/#input",
            "text": "Every ML model requires input, using which it generates output.  Input  is represented by first block in Showcase project workflow. For MOJO and PMML projects, Showcase parses the model and automatically extracts and pre-configures model inputs. For Python models, this needs to be specified explicitly. At any point of time, we can modify model inputs by pressing the  Configure  button inside input block.   Input configuration allows us to specify various input items for the model. In case of MOJO and PMML, model input line items are auto derived from model file and cannot be added/removed. But we can alter certain properties of model input line items. Each input line item has following properties   Variable  - This is the name which model actually recognises. In case of H2O and PMML, this is burnt in model and is a read only field. This property is used in Showcase input validation. In case a scoring request comes with a missing input or a mis-spelt input, request is rejected with Validation error.  Name  - This is a user friendly name for the model input item. We can specify whatever we want. This is the name shown to end user on  Scoring UI .  Important  - This allows user to specify if this particular input is important. If some fields are marked as important, Scoring UI automatically shows only important inputs on first load. Users can press  View All  to see all inputs. It is useful for models that have large number of inputs and we want to make it easy of people to test the model with important inputs using Scoring UI.  Description  - This is again used for Scoring UI. Users testing the model via Scoring UI, can see this description to know more about this input.   Field Type  - User can specify field to be one of the following types   Text  Multi Line Text  Enum  Integer  Number   This property is used in Showcase input validation. In case a socoring request comes with inputs not following the type specified, request is rejected with Validation error. This property is also used to design input form for Socring UI.    Customize  - This property is used to add more validation rules for inputs. Enum categories can be defined for Enum input types. For integer/number input types, we can specify the range within which input should fall. This property is also used for input validation and for designing input form for Scoring UI.",
            "title": "Input"
        },
        {
            "location": "/showcase/uploadmodels/configure/#output",
            "text": "Output  from the model, is represented by the last block in Showcase project workflow. For MOJO and PMML projects, Showcase parses the model and automatically extracts and pre-configures model output. For Python models, this needs to be specified explicitly. At any point of time, we can modify model output schema by pressing the  Configure  button inside output block.   Output configuration allows us to specify various output elements for the model. In case of MOJO and PMML, model output line items are auto derived from model file and cannot be added/removed. But we can alter certain properties of model output line items. Output line item properties are similar to Input line item properties described above. We have an additional column for  Advanced Settings  in output. Details about this setting is covered under  customizations for Scoring UI .",
            "title": "Output"
        },
        {
            "location": "/showcase/uploadmodels/preprocessing/",
            "text": "We can enable pre-processing on user input before we feed it into the model for evaluation.\n\n\nNote\n:\n \nThis\n \nconfiguration\n \ndoes\n \nnot\n \napply\n \nfor\n \nNotebook\n \nproject\n \ntype\n.\n\n\n\n\n\n\n\nFigure 1 - Pre-processing workflow\n\n\nPreprocessing can be enabled by going into Showcase project Settings and enabling option \nNeed Preprocessing\n\n\n\n\nFigure 2 - Enable pre-processing\n\n\nOnce pre-processing is enabled, project pipleline workflow changes as per Figure 1 above. Two new blocks get added before Model Input\n\n\nRaw Input\n\u00b6\n\n\n\n\nFigure 3 - Raw input configuration\n\n\nThis is the main input to the project. Raw input is completely defined by user. We can press Configure on Raw input block to bring up its configuration. Here we can add / remove input line items and configure its Name, Description, Type, Enum types, ranges, etc.\n\n\nPreprocessing\n\u00b6\n\n\n\n\nFigure 4 - Configure preprocess code\n\n\nWe can specify pre-processing python code that transforms raw input into a form that can be consumed by our model. Press \nPreprocess Code\n button on Preprocessing block to open the code editor.\n\n\nCode\n\u00b6\n\n\n\n\nFigure 5 - Configure preprocess code\n\nCode editor scaffolds a \npreprocess\n python function that needs to be filled up. Input to this function is a python object \ndata\n, an array containing raw inputs. We need to paste our pre-processing code inside the predict function. Output from this function needs to be another array which will be input into the model.\n\n\nWe are free to include any other helper code outside the preprocess function like importing other libs, defining other functions and classes, declaring and initializing global variables, etc. \n\n\nLibs\n\u00b6\n\n\n\n\nFigure 6 - Configure preprocess libs\n\nIn case your pre-processing code requires other python libraries, you can add them here. Go to the text field at the bottom of code editor, which says \nAdd pip packages\n, type in your pip package name and press Enter. Add as many packages as you want. Make sure to press Save button to ensure these dependency packages list is saved.\n\n\nTest\n\u00b6\n\n\n\n\nFigure 7 - Test preprocess code\n\nClouderizer console allows us to test our code within the code editor itself. It requires us to get debugging setup. Please follow instructions \nhere\n to set this up. Once your setup is proper, code editor shows \nConnected to Kernel\n message on top left. Once our code editor shows that it is connected to kernel, we can press \nRun\n button at bottom of the code editor to run the code. Make sure you fill the input data object with some dummy raw input data for testing. Output shall appear on the right under Output. Once you are satisfied with your pre-processing code, press Save and Upload button to save it.\n\n\n\n\nTip\n\n\nMake sure to press Save button as you are modifying your code and dependency package list to ensure your changes are saved.\n\n\n\n\nModel Input\n\u00b6\n\n\n\n\nFigure 8 - Model input\n\nModel input is what is fed to model for scoring. Output from preprocess code (what we return from preprocess function above) should be in the format of model input scheam. We can press \nConfigure\n button on model input block to access its configuration. In case of PMML and MOJO projects, model input line items are fixed and read only. We can just modify its name, description and field types if needed. In case of Python projects, complete model input schema is configurable. We can add / remove input line items as per our project specifications. Model input schema is used in Showcase input validation. Which means if our pre-process code is not able to produce output as defined in model input config here, request fails input validation.",
            "title": "Preprocessing"
        },
        {
            "location": "/showcase/uploadmodels/preprocessing/#raw-input",
            "text": "Figure 3 - Raw input configuration  This is the main input to the project. Raw input is completely defined by user. We can press Configure on Raw input block to bring up its configuration. Here we can add / remove input line items and configure its Name, Description, Type, Enum types, ranges, etc.",
            "title": "Raw Input"
        },
        {
            "location": "/showcase/uploadmodels/preprocessing/#preprocessing",
            "text": "Figure 4 - Configure preprocess code  We can specify pre-processing python code that transforms raw input into a form that can be consumed by our model. Press  Preprocess Code  button on Preprocessing block to open the code editor.",
            "title": "Preprocessing"
        },
        {
            "location": "/showcase/uploadmodels/preprocessing/#code",
            "text": "Figure 5 - Configure preprocess code \nCode editor scaffolds a  preprocess  python function that needs to be filled up. Input to this function is a python object  data , an array containing raw inputs. We need to paste our pre-processing code inside the predict function. Output from this function needs to be another array which will be input into the model.  We are free to include any other helper code outside the preprocess function like importing other libs, defining other functions and classes, declaring and initializing global variables, etc.",
            "title": "Code"
        },
        {
            "location": "/showcase/uploadmodels/preprocessing/#libs",
            "text": "Figure 6 - Configure preprocess libs \nIn case your pre-processing code requires other python libraries, you can add them here. Go to the text field at the bottom of code editor, which says  Add pip packages , type in your pip package name and press Enter. Add as many packages as you want. Make sure to press Save button to ensure these dependency packages list is saved.",
            "title": "Libs"
        },
        {
            "location": "/showcase/uploadmodels/preprocessing/#test",
            "text": "Figure 7 - Test preprocess code \nClouderizer console allows us to test our code within the code editor itself. It requires us to get debugging setup. Please follow instructions  here  to set this up. Once your setup is proper, code editor shows  Connected to Kernel  message on top left. Once our code editor shows that it is connected to kernel, we can press  Run  button at bottom of the code editor to run the code. Make sure you fill the input data object with some dummy raw input data for testing. Output shall appear on the right under Output. Once you are satisfied with your pre-processing code, press Save and Upload button to save it.   Tip  Make sure to press Save button as you are modifying your code and dependency package list to ensure your changes are saved.",
            "title": "Test"
        },
        {
            "location": "/showcase/uploadmodels/preprocessing/#model-input",
            "text": "Figure 8 - Model input \nModel input is what is fed to model for scoring. Output from preprocess code (what we return from preprocess function above) should be in the format of model input scheam. We can press  Configure  button on model input block to access its configuration. In case of PMML and MOJO projects, model input line items are fixed and read only. We can just modify its name, description and field types if needed. In case of Python projects, complete model input schema is configurable. We can add / remove input line items as per our project specifications. Model input schema is used in Showcase input validation. Which means if our pre-process code is not able to produce output as defined in model input config here, request fails input validation.",
            "title": "Model Input"
        },
        {
            "location": "/showcase/uploadmodels/prediction/",
            "text": "Once we create the project and configure its inputs and outputs, we are all set. PMML and MOJO projects are completely code-less. Which means we don't need to specify any code to deploy these models. Clouderizer's PMML and MOJO scoring engine takes care of that.\n\n\nIn case of Python projects, users need to provide a prediction code snippet, which uses the python pickle file and performs prediction. Showcase takes this snippet and integrates it with the deployment pipeline, offering end to end robust, scalable, secure, manageable deployment.\n\n\n\n\nFigure 1 - Prediction code button\n\n\nTo configure the prediction code, press \nPrediction Code\n button under Model block. This brings up console code editor.\n\n\nCode\n\u00b6\n\n\n\n\nFigure 2 - Prediction code\n\nCode editor scaffolds a \npredict\n python function that needs to be filled up. Input to this function is a python object \ndata\n, an array containing model inputs. Scaffoled code also includes lines which loads the model pickle file and offers it as a python object. We need to paste our prediction code inside the predict function. Output from this function needs to be another array matching schema for model output.\n\n\nWe are free to include any other helper code outside the predict function like importing other libs, defining other functions and classes, declaring and initializing global variables, etc. \n\n\nLibs\n\u00b6\n\n\n\n\nFigure 3 - Configure preprocess libs\n\nIn case your pre-processing code requires other python libraries, you can add your dependency packages here. Go to the text field at the bottom of code editor, which says \nAdd pip packages\n, type in your pip package name and press Enter. Add as many packages as you want. Make sure to press Save button to ensure these dependency packages list is saved.\n\n\nTest\n\u00b6\n\n\n\n\nFigure 4 - Test preprocess code\n\nClouderizer console allows us to test our code within the code editor itself. It requires us to get debugging setup. Please follow instructions \nhere\n to set this up. Once your setup is proper, code editor shows \nConnected to Kernel\n message on top left. Once our code editor shows that it is connected to kernel, we can press \nRun\n button at bottom of the code editor to run the code. Make sure you fill the input data object with some dummy input data for testing. Output shall appear on the right under Output. Once you are satisfied with your pre-processing code, press Save and Upload button to save it.\n\n\n\n\nTip\n\n\nMake sure to press Save button as you are modifying your code and dependency package list to ensure your changes are saved.",
            "title": "Prediction"
        },
        {
            "location": "/showcase/uploadmodels/prediction/#code",
            "text": "Figure 2 - Prediction code \nCode editor scaffolds a  predict  python function that needs to be filled up. Input to this function is a python object  data , an array containing model inputs. Scaffoled code also includes lines which loads the model pickle file and offers it as a python object. We need to paste our prediction code inside the predict function. Output from this function needs to be another array matching schema for model output.  We are free to include any other helper code outside the predict function like importing other libs, defining other functions and classes, declaring and initializing global variables, etc.",
            "title": "Code"
        },
        {
            "location": "/showcase/uploadmodels/prediction/#libs",
            "text": "Figure 3 - Configure preprocess libs \nIn case your pre-processing code requires other python libraries, you can add your dependency packages here. Go to the text field at the bottom of code editor, which says  Add pip packages , type in your pip package name and press Enter. Add as many packages as you want. Make sure to press Save button to ensure these dependency packages list is saved.",
            "title": "Libs"
        },
        {
            "location": "/showcase/uploadmodels/prediction/#test",
            "text": "Figure 4 - Test preprocess code \nClouderizer console allows us to test our code within the code editor itself. It requires us to get debugging setup. Please follow instructions  here  to set this up. Once your setup is proper, code editor shows  Connected to Kernel  message on top left. Once our code editor shows that it is connected to kernel, we can press  Run  button at bottom of the code editor to run the code. Make sure you fill the input data object with some dummy input data for testing. Output shall appear on the right under Output. Once you are satisfied with your pre-processing code, press Save and Upload button to save it.   Tip  Make sure to press Save button as you are modifying your code and dependency package list to ensure your changes are saved.",
            "title": "Test"
        },
        {
            "location": "/showcase/uploadmodels/postprocessing/",
            "text": "Coming Soon\n\u00b6",
            "title": "Postprocessing"
        },
        {
            "location": "/showcase/uploadmodels/postprocessing/#coming-soon",
            "text": "",
            "title": "Coming Soon"
        },
        {
            "location": "/showcase/uploadmodels/debuggingsetup/",
            "text": "Clouderizer Showcase allows us to test our preprocessing, prediction and postprocessing code in the console itself. This testing requires a debugging env setup on your local machine. \n\n\nPre-requisites\n\n\n\n\nLinux or Mac machines with Docker installed and running.\n\n\nMin. 4GB of RAM\n\n\n\n\nLets go over the steps needed for this setup.\n\n\n\n\n\n\nOpen your Showcase project and launch code editor for any of your pre-processing or prediction block.\n\n\n\nFigure 1 - Prediction code button\n\n\n\n\n\n\nFirst time, you see following message at the top of code editor window.\n\n\n\nFigure 2 - Setup instructions\n\n\n\n\n\n\nCopy the curl command by pressing the copy button at the end of text box.\n\n\n\nFigure 3 - Copy command\n\n\n\n\n\n\nOpen a terminal on your machine and paste the command copied above and press Enter.\n\n\n\nFigure 4 - Run on terminal\n\n\n\n\n\n\nThis should start the dev docker env on your machine. Once docker is up and running, switch back to Showcase console code editor view. It should detect the dev environment and show \nConnected to Kernel\n\n\n\n\nFigure 5 - Connected to kernel\n\n\n\n\n\n\nYou can now test your code by pressing Run button. Output should appear on the right.\n\n\n\n\n\n\nIn case you have added some dependency pip package to your pre-process or prediction code, and want to install the same package in your debugging env as well, press the install button on the right of pip packages text list. You can see the progress of package installation from the output view.\n\n\n\nFigure 6 - Install dependencies",
            "title": "Debugging Setup"
        },
        {
            "location": "/showcase/deploymodels/",
            "text": "Once clouderizer project is created and fully configured, we are all set for deployment. We can trigger deployment from CLI or from our web console. For CLI instructions please refer \nhere\n\n\nFor deploying from web console, go to your project page and press the Deploy button on top right.\n\n\n\n\nFigure 1 - Deploy model button\n\n\nFollowing deployment options are available\n\n\n\n\nStandard Memory config - 2GB Memory\n\n\nHigh Memory config - 6GB Memory\n\n\nGPU Config - 16GB GPU Memory, 30GB System Memory\n\n\n\n\nSelect the desired infra option and press OK.\n\n\nThis will trigger start the deployment process, where your project is bundled in a container and pushed on Clouderizer Serverless infrastructure.\n\n\nDeployment Status\n\u00b6\n\n\nOnce deployment is started using any of the methods above, status on the project page updates to indicate deployment is in progress. We can see details about the progress from bottom left portion of screen.\n\n\n\n\nFigure 8 - Deployment in progress\n\n\nWhen deployment is complete, status changes to Running state and we can see deployed project's URL in the bottom left portion of screen. This URL can be used for invoking the serverless endpoint. More details on scoring can be found \nhere\n and \nhere\n.\n\n\n\n\nFigure 9 - Deployment complete\n\n\nScalable Serverless Deployment\n\u00b6\n\n\nClouderizer offers a scalable serverless infrastructure, which means it auto scales your deployment to multiple replicas and nodes in case of high load and it auto scales down to 0 in case of no traffic. Billing for Clouderizer infra is per-second execution time, which significantly saves on idle capacity charges.",
            "title": "Deploy"
        },
        {
            "location": "/showcase/deploymodels/#deployment-status",
            "text": "Once deployment is started using any of the methods above, status on the project page updates to indicate deployment is in progress. We can see details about the progress from bottom left portion of screen.   Figure 8 - Deployment in progress  When deployment is complete, status changes to Running state and we can see deployed project's URL in the bottom left portion of screen. This URL can be used for invoking the serverless endpoint. More details on scoring can be found  here  and  here .   Figure 9 - Deployment complete",
            "title": "Deployment Status"
        },
        {
            "location": "/showcase/deploymodels/#scalable-serverless-deployment",
            "text": "Clouderizer offers a scalable serverless infrastructure, which means it auto scales your deployment to multiple replicas and nodes in case of high load and it auto scales down to 0 in case of no traffic. Billing for Clouderizer infra is per-second execution time, which significantly saves on idle capacity charges.",
            "title": "Scalable Serverless Deployment"
        },
        {
            "location": "/showcase/scoremodels/restfulapis/",
            "text": "Once Showcase project is deployed successfully, Clouderizer exposes REST endpoints for it. These REST endpoints can be called by any external application to score their data using this model.\n\n\nREST endpoint URL is available in the bottom left screen of a running project\n\n\n\n\nFigure 1 - REST Enpoint URL\n\n\nNotebooks\n\u00b6\n\n\nAsync Endpoints\n\u00b6\n\n\nNotebook deployments support Async serverless endpoints. This is useful as often Notebook execution is a long running operation. Async endpoints can be invoked via any http client like curl. Caller can specify a callback URL (webhook) in its http invocation using http header \nX-callback-url\n. Serverless function immediately replies back with 202 request accepted response code and continue to execute the notebook. Once notebook execution is complete, the callback URL (webhook) is called with the result. In addition to this webhook, output of the invocation can also be viewed under \nInvocation History\n on Clouderizer console.\n\n\nModels\n\u00b6\n\n\nFollowing are the handlers available model deployment REST endpoint\n\n\n/predict\n\u00b6\n\n\nThis is the api which allows any external application to submit input and get back scored output. Showcase console offers sample code for invoking this handler in Node.js, Python, Java and C#.\n\n\n\nFigure 2 - REST Sample Code\n\n\nMethod Type\n\u00b6\n\n\nPOST\n\n\nInput Payload\n\u00b6\n\n\nModel input need to be submitted as a form payload with field name csv. Value of this field should be a comman separated values of all model inputs (order of inputs should be exactly same as order defined in Model project page)\n\n\nOutput Payload\n\u00b6\n\n\nFollowing is the convention for output for various data types\n\n\nInteger, Whole and Text\n\u00b6\n\n\nOutput is an array of all output variables defined in Model project page under Output.\n\n\n{\"\n\":\n,..., \"responseTime\":\"\n\", \"success\":\"true\"}\n\n\nEnums\n\u00b6\n\n\nEnum outputs are returned in json format with following schema\n\n\n{\"label\":\n, \"classprobabilities\":[\n, \n,...], \"responseTime\":\"\n\", \"success\",\"\n\"}\n\n\nErrors\n\u00b6\n\n\n/feedback\n\u00b6\n\n\nThis is the api using which an external application or end user can give feedback about the model performance.\n\n\nMethod Type\n\u00b6\n\n\nPOST\n\n\nInput Payload\n\u00b6\n\n\nOutput Payload\n\u00b6\n\n\nErrors\n\u00b6",
            "title": "RESTful APIs"
        },
        {
            "location": "/showcase/scoremodels/restfulapis/#notebooks",
            "text": "",
            "title": "Notebooks"
        },
        {
            "location": "/showcase/scoremodels/restfulapis/#async-endpoints",
            "text": "Notebook deployments support Async serverless endpoints. This is useful as often Notebook execution is a long running operation. Async endpoints can be invoked via any http client like curl. Caller can specify a callback URL (webhook) in its http invocation using http header  X-callback-url . Serverless function immediately replies back with 202 request accepted response code and continue to execute the notebook. Once notebook execution is complete, the callback URL (webhook) is called with the result. In addition to this webhook, output of the invocation can also be viewed under  Invocation History  on Clouderizer console.",
            "title": "Async Endpoints"
        },
        {
            "location": "/showcase/scoremodels/restfulapis/#models",
            "text": "Following are the handlers available model deployment REST endpoint",
            "title": "Models"
        },
        {
            "location": "/showcase/scoremodels/restfulapis/#predict",
            "text": "This is the api which allows any external application to submit input and get back scored output. Showcase console offers sample code for invoking this handler in Node.js, Python, Java and C#.  Figure 2 - REST Sample Code",
            "title": "/predict"
        },
        {
            "location": "/showcase/scoremodels/restfulapis/#method-type",
            "text": "POST",
            "title": "Method Type"
        },
        {
            "location": "/showcase/scoremodels/restfulapis/#input-payload",
            "text": "Model input need to be submitted as a form payload with field name csv. Value of this field should be a comman separated values of all model inputs (order of inputs should be exactly same as order defined in Model project page)",
            "title": "Input Payload"
        },
        {
            "location": "/showcase/scoremodels/restfulapis/#output-payload",
            "text": "Following is the convention for output for various data types",
            "title": "Output Payload"
        },
        {
            "location": "/showcase/scoremodels/restfulapis/#integer-whole-and-text",
            "text": "Output is an array of all output variables defined in Model project page under Output.  {\" \": ,..., \"responseTime\":\" \", \"success\":\"true\"}",
            "title": "Integer, Whole and Text"
        },
        {
            "location": "/showcase/scoremodels/restfulapis/#enums",
            "text": "Enum outputs are returned in json format with following schema  {\"label\": , \"classprobabilities\":[ ,  ,...], \"responseTime\":\" \", \"success\",\" \"}",
            "title": "Enums"
        },
        {
            "location": "/showcase/scoremodels/restfulapis/#errors",
            "text": "",
            "title": "Errors"
        },
        {
            "location": "/showcase/scoremodels/restfulapis/#feedback",
            "text": "This is the api using which an external application or end user can give feedback about the model performance.",
            "title": "/feedback"
        },
        {
            "location": "/showcase/scoremodels/restfulapis/#method-type_1",
            "text": "POST",
            "title": "Method Type"
        },
        {
            "location": "/showcase/scoremodels/restfulapis/#input-payload_1",
            "text": "",
            "title": "Input Payload"
        },
        {
            "location": "/showcase/scoremodels/restfulapis/#output-payload_1",
            "text": "",
            "title": "Output Payload"
        },
        {
            "location": "/showcase/scoremodels/restfulapis/#errors_1",
            "text": "",
            "title": "Errors"
        },
        {
            "location": "/showcase/scoremodels/scoringui/introduction/",
            "text": "Clouderizer Showcase offers an out of box, autogenerated \nScoring UI\n for all deployed models. It is a modern web interface which allows anyone to feed in model inputs and get back scored output. It also allows users to give feedback about how they feel model is performing. This is an excellent tool for \n\n\n\n\nTesting and Validation of model from external teams\n\n\nShowcasing model to clients, beta customers, senior management\n\n\nPlaying around with model\n\n\n\n\n\n\nFigure 1 - Sample Scoring UI\n\n\nScoring UI is built on top of modern web interface technology. Its fully responsive and works well on both desktop and mobiles.\n\n\n\n\nFigure 2 - Scoring UI on mobile\n\n\nScoring UI endpoint can be made public, such that it can be shared with people who don't have Clouderizer account.\n\n\nBase URL for project REST endpoint loads this application. Easiest way to access it would be to click on the URL on the bottom left screen of running Showcase project.",
            "title": "Introduction"
        },
        {
            "location": "/showcase/scoremodels/scoringui/formbasedscoring/",
            "text": "Most simplistic way to use Scoring UI is to use the input form. \n\n\nInput\n\u00b6\n\n\nScoring UI dynamically generates input form based on raw inputs defined in Showcase project. \n\n\n\nFigure 1 - Sample Input Form\n\n\nForm is equipped to handle input validation like integer ranges, enum types, etc, as defined in the Showcase project. Also description for each input field is available as a tool tip to the \"i\" icon at the end of every form line.\n\n\nIn case user has marked few input fields as important, Scoring UI presents just the important fields. This is particularly useful for models which have large number of inputs. Limiting the form to just the most important fields makes it easy for anyone to try out the model. In case someone wishes to input all the fields, one can toggle to \"View All\" view from top right corner of the form.\n\n\nOutput\n\u00b6\n\n\nOnce input form is submitted, Scoring UI calls the RESTful API /predict for the Showcase project to get back the prediction. Output from the REST endpoint is displayed on the right portion of Scoring UI.\n\n\n\nFigure 2 - Sample Output\n\n\nOutput as defined under Showcase project is displayed under section \"Output\" in figure above.\n\n\nFor classification models, class probabilities are also displayed as shown in Fig 2.\n\n\nTo make output realistic and user friendly, we can define icons, images, gifs for enum outputs along with some custom messages. Details about how to configure these custom outputs can be found \nhere\n\n\nFeedback\n\u00b6\n\n\n\n\nFigure 3 - Sample Feedback\n\n\nAlong with output variables, Scoring UI also displays \nResponse Time\n taken by the model to make the prediction. This is a good indicator about how efficiently model as well as the infrastructure where model is deployed, is performing. Users or testers can also give feedback with a thumbs up or thumbs down, for how they feel about the output. This helps build up feedback analytics about model performance. More details about model monitoring and analytics can be found \nhere",
            "title": "Form based Scoring"
        },
        {
            "location": "/showcase/scoremodels/scoringui/formbasedscoring/#input",
            "text": "Scoring UI dynamically generates input form based on raw inputs defined in Showcase project.   Figure 1 - Sample Input Form  Form is equipped to handle input validation like integer ranges, enum types, etc, as defined in the Showcase project. Also description for each input field is available as a tool tip to the \"i\" icon at the end of every form line.  In case user has marked few input fields as important, Scoring UI presents just the important fields. This is particularly useful for models which have large number of inputs. Limiting the form to just the most important fields makes it easy for anyone to try out the model. In case someone wishes to input all the fields, one can toggle to \"View All\" view from top right corner of the form.",
            "title": "Input"
        },
        {
            "location": "/showcase/scoremodels/scoringui/formbasedscoring/#output",
            "text": "Once input form is submitted, Scoring UI calls the RESTful API /predict for the Showcase project to get back the prediction. Output from the REST endpoint is displayed on the right portion of Scoring UI.  Figure 2 - Sample Output  Output as defined under Showcase project is displayed under section \"Output\" in figure above.  For classification models, class probabilities are also displayed as shown in Fig 2.  To make output realistic and user friendly, we can define icons, images, gifs for enum outputs along with some custom messages. Details about how to configure these custom outputs can be found  here",
            "title": "Output"
        },
        {
            "location": "/showcase/scoremodels/scoringui/formbasedscoring/#feedback",
            "text": "Figure 3 - Sample Feedback  Along with output variables, Scoring UI also displays  Response Time  taken by the model to make the prediction. This is a good indicator about how efficiently model as well as the infrastructure where model is deployed, is performing. Users or testers can also give feedback with a thumbs up or thumbs down, for how they feel about the output. This helps build up feedback analytics about model performance. More details about model monitoring and analytics can be found  here",
            "title": "Feedback"
        },
        {
            "location": "/showcase/scoremodels/scoringui/bulkscoring/",
            "text": "Figure 1 - Download CSV Template\n\n\nIn case we wish to make bulk predictions on our model, Scoring UI gives a very easy to use and intuitive interface to do that. Testers or users need not write any code or draft curl commands. They can just upload a CSV with a list of inputs and get back the output in one shot.\n\n\n\n\nFigure 2 - Editing CSV\n\n\nWe can download a template CSV by pressing button #1 from Fig.1 above. This csv can be opened and edited in any text editor or spreadsheet editor. It contains the header rows. We can now insert rows of input data, save the file and upload it using the button #2 in Fig. 1 above.  \n\n\n\n\nFigure 3 - Review Inputs\n\n\nOnce uploaded, showcase allows us to review the bulk input rows and make corrections if needed. Press Save once all review changes are done. And then press Submit to trigger bulk prediction on this data. This will send this batch request to the deployed Showcase project, predict each individual rows from the project model and return back the scored data.\n\n\nOnce output is ready, we can press View Results to view them.\n\n\n\n\nFigure 4 - View Results\n\n\nThis opens up the list view with all input rows. All output columns are appended at the end. There is one additional column for feedback. User can give thumbs up or down for the individual line predictions here.",
            "title": "Bulk Scoring"
        },
        {
            "location": "/showcase/scoremodels/scoringui/customizations/",
            "text": "Figure 1 - Scoring UI Customization Elements\n\n\nShowcase offers following customization and branding options for the Scoring UI\n\n\nTitle and Description\n\u00b6\n\n\n\n\nFigure 2 - Scoring UI Title & Description\n\n\nProject title and description can be configured from Showcase project page. Clicking on the Title name allows to modify it. Similarly clicking on the description text, allows us to modify it. Once modified, we can press Save button to commit it.\n\n\nBanner Image\n\u00b6\n\n\n\n\nFigure 3 - Scoring UI Banner Image\n\n\nBanner image can be updated from Project page -> Settings. We can upload and save the banner image. \n\n\nEnum Output Images\n\u00b6\n\n\n\n\nFigure 4 - Scoring UI Output Icons\n\n\n\n\nFigure 5 - Scoring UI Output Icons\n\n\nOutput on the Scoring UI can be complimented with intuitive icons / images / gifs to offer an impressive presentation. This can be configured from Showcase project page -> Output -> Configure. All output variables of type Enums have an additional button \nConfigure\n in their row (Fig 4). Pressing configure, gives option to select an image and specify a text (punch line) for each Enum state (Fig 5). Images can be either uploaded or selected from a builtin gallery. Once configured, save the setting.\n\n\nAll the above customization options, once configured, get loaded by Scoring UI on next browser refresh.",
            "title": "Customizations"
        },
        {
            "location": "/showcase/scoremodels/scoringui/customizations/#title-and-description",
            "text": "Figure 2 - Scoring UI Title & Description  Project title and description can be configured from Showcase project page. Clicking on the Title name allows to modify it. Similarly clicking on the description text, allows us to modify it. Once modified, we can press Save button to commit it.",
            "title": "Title and Description"
        },
        {
            "location": "/showcase/scoremodels/scoringui/customizations/#banner-image",
            "text": "Figure 3 - Scoring UI Banner Image  Banner image can be updated from Project page -> Settings. We can upload and save the banner image.",
            "title": "Banner Image"
        },
        {
            "location": "/showcase/scoremodels/scoringui/customizations/#enum-output-images",
            "text": "Figure 4 - Scoring UI Output Icons   Figure 5 - Scoring UI Output Icons  Output on the Scoring UI can be complimented with intuitive icons / images / gifs to offer an impressive presentation. This can be configured from Showcase project page -> Output -> Configure. All output variables of type Enums have an additional button  Configure  in their row (Fig 4). Pressing configure, gives option to select an image and specify a text (punch line) for each Enum state (Fig 5). Images can be either uploaded or selected from a builtin gallery. Once configured, save the setting.  All the above customization options, once configured, get loaded by Scoring UI on next browser refresh.",
            "title": "Enum Output Images"
        },
        {
            "location": "/showcase/managemodels/monitornotebooks/",
            "text": "Clouderizer Showcase offers Invocation History for all Notebook projects\n\n\nInvocations\n\u00b6\n\n\nShowcase stores each requests hitting the serverless function in a scalable time series storage. This list is available to us from \nInvocations\n. Important columns from this list are\n\n\n\n\nTimestamp - Time of request\n\n\nInput - # of input parameters for the request\n\n\nOutput - # of output parameters returned for the request\n\n\nStatus - Whether notebook execution complete without any errors. In case of errors, detail of error is shown under details.\n\n\n\n\nView Request Details\n\u00b6\n\n\nClicking on \nMore details\n button on individual row from the list, shows the details of input parameters sent for the request and corresponding output generated. All artifacts in http requests (files and fields) are archived in Invocation history. Similarly any output generated by the notebook is also archived here. These files can be downloaded from this view. In addition executed notebook is also avialable for viewing and download from here.",
            "title": "Monitor Notebooks"
        },
        {
            "location": "/showcase/managemodels/monitornotebooks/#invocations",
            "text": "Showcase stores each requests hitting the serverless function in a scalable time series storage. This list is available to us from  Invocations . Important columns from this list are   Timestamp - Time of request  Input - # of input parameters for the request  Output - # of output parameters returned for the request  Status - Whether notebook execution complete without any errors. In case of errors, detail of error is shown under details.",
            "title": "Invocations"
        },
        {
            "location": "/showcase/managemodels/monitornotebooks/#view-request-details",
            "text": "Clicking on  More details  button on individual row from the list, shows the details of input parameters sent for the request and corresponding output generated. All artifacts in http requests (files and fields) are archived in Invocation history. Similarly any output generated by the notebook is also archived here. These files can be downloaded from this view. In addition executed notebook is also avialable for viewing and download from here.",
            "title": "View Request Details"
        },
        {
            "location": "/showcase/managemodels/monitormodels/",
            "text": "Figure 1 - Analytics\n\n\nClouderizer Showcase offers an analytics dashboard to monitor deployed model performance and other statistics. This dashboard can be accessed from \nAnalytics\n button from Showcase project page. Analytics Dashboard offers following\n\n\nCharts\n\u00b6\n\n\nTop of the Analytics dashboard shows statistical charts showing historical trends. \n\n\nRequest Count\n\u00b6\n\n\nThis shows rate of requests hitting Showcase project deployment server. This is a good indicator for monitoring request traffic hitting the model server.\n\n\nPrediction Response Time\n\u00b6\n\n\nThis shows trend for average response time taken by the model to make prediction. It is a good indicator for monitoring model compute performance.\n\n\nFeedback\n\u00b6\n\n\nThis shows trend for positive and negative feedback from users or integrated applications for model prediction. This again is a good indicator to keep a track on model performance and detect any deterioration in model accuracy.\n\n\nHistorical Request List\n\u00b6\n\n\nShowcase stores each requests hitting the model server in a scalable time series storage. This list is available to us from Analytics dashboard. Important columns from this list are\n\n\n\n\nTimestamp - Time of request\n\n\nResponse Time - Time taken for model to make prediction\n\n\nFeedback - User feedback on the model prediction\n\n\nPrediction - whether prediction was done successfully or not. In case some error occured in making the prediction, this staus changes to Error.\n\n\n\n\nView Request Details\n\u00b6\n\n\n\n\nFigure 2 - View Request Details\n\n\nClicking on \nMore details\n button on individual row from the list, shows the details of input parameters sent for the request and corresponding output scored by the model.\n\n\nDownload CSV\n\u00b6\n\n\n\n\nFigure 3 - Download CSV\n\n\nThe historical request list can be downloaded as a CSV using the \nDownload CSV\n button on the top right corner of the Analytics page. This is very useful to prepare an addendum to the labelled dataset for re-training the model.",
            "title": "Monitor Models"
        },
        {
            "location": "/showcase/managemodels/monitormodels/#charts",
            "text": "Top of the Analytics dashboard shows statistical charts showing historical trends.",
            "title": "Charts"
        },
        {
            "location": "/showcase/managemodels/monitormodels/#request-count",
            "text": "This shows rate of requests hitting Showcase project deployment server. This is a good indicator for monitoring request traffic hitting the model server.",
            "title": "Request Count"
        },
        {
            "location": "/showcase/managemodels/monitormodels/#prediction-response-time",
            "text": "This shows trend for average response time taken by the model to make prediction. It is a good indicator for monitoring model compute performance.",
            "title": "Prediction Response Time"
        },
        {
            "location": "/showcase/managemodels/monitormodels/#feedback",
            "text": "This shows trend for positive and negative feedback from users or integrated applications for model prediction. This again is a good indicator to keep a track on model performance and detect any deterioration in model accuracy.",
            "title": "Feedback"
        },
        {
            "location": "/showcase/managemodels/monitormodels/#historical-request-list",
            "text": "Showcase stores each requests hitting the model server in a scalable time series storage. This list is available to us from Analytics dashboard. Important columns from this list are   Timestamp - Time of request  Response Time - Time taken for model to make prediction  Feedback - User feedback on the model prediction  Prediction - whether prediction was done successfully or not. In case some error occured in making the prediction, this staus changes to Error.",
            "title": "Historical Request List"
        },
        {
            "location": "/showcase/managemodels/monitormodels/#view-request-details",
            "text": "Figure 2 - View Request Details  Clicking on  More details  button on individual row from the list, shows the details of input parameters sent for the request and corresponding output scored by the model.",
            "title": "View Request Details"
        },
        {
            "location": "/showcase/managemodels/monitormodels/#download-csv",
            "text": "Figure 3 - Download CSV  The historical request list can be downloaded as a CSV using the  Download CSV  button on the top right corner of the Analytics page. This is very useful to prepare an addendum to the labelled dataset for re-training the model.",
            "title": "Download CSV"
        },
        {
            "location": "/showcase/managemodels/updatenotebooks/",
            "text": "Notebook projects can be updated with newer versions of notebook at any time. Clouderizer ensures rolling update with neglegible downtime.\n\n\n\n\nTip\n\n\nFor CLI instructions for updating notebooks in a deployment, please refer \nhere\n\n\n\n\nSteps\n\u00b6\n\n\n\n\n\n\nLogin to the console and go to the project page.\n\n\n\n\n\n\nPress \nUpdate\n button from the central block.\n\n\n\n\n\n\nUpload the new notebook (and if needed new requirements.txt) file  and press Finish.\n\n\n\n\n\n\nOnce upload is completed, notebook is updated in the project. In order to push this new update to your existing deployment, press \nDeploy\n button from top right corner.",
            "title": "Update Notebooks"
        },
        {
            "location": "/showcase/managemodels/updatenotebooks/#steps",
            "text": "Login to the console and go to the project page.    Press  Update  button from the central block.    Upload the new notebook (and if needed new requirements.txt) file  and press Finish.    Once upload is completed, notebook is updated in the project. In order to push this new update to your existing deployment, press  Deploy  button from top right corner.",
            "title": "Steps"
        },
        {
            "location": "/showcase/managemodels/updatemodels/",
            "text": "Model projects can be updated with new model files, preprocessing, prediction, post processing code, at any time, even if they are currently deployed and live. Clouderizer ensures rolling update with neglegible downtime.\n\n\n\n\nTip\n\n\nFor CLI instructions for updating models in a deployment, please refer \nhere\n\n\n\n\nUpdating Model File\n\u00b6\n\n\n\n\nFigure 1 - Update Model File\n\n\nModel file can be uploaded using \nUpdate Model\n button on the Showcase project page under Model block.\n\n\nUpdating Pre-processing Code\n\u00b6\n\n\nPre-processing code can be updated any time by going to the code editor using \nPre-processing Code\n button under Pre-processing block. We can make our changes, test them by running it. Once finalized, we should \nSave\n and \nUpload\n the updated pre-processing code. Project needs to be re-deployed for the new changes to take effect.\n\n\nUpdating Prediction Code\n\u00b6\n\n\nPrediction code can be updated any time by going to the code editor using \nPrediction Code\n button under Model block. We can make our changes, test them by running it. Once finalized, we should \nSave\n and \nUpload\n the updated prediction code. Project needs to be re-deployed for the new changes to take effect.",
            "title": "Update Models"
        },
        {
            "location": "/showcase/managemodels/updatemodels/#updating-model-file",
            "text": "Figure 1 - Update Model File  Model file can be uploaded using  Update Model  button on the Showcase project page under Model block.",
            "title": "Updating Model File"
        },
        {
            "location": "/showcase/managemodels/updatemodels/#updating-pre-processing-code",
            "text": "Pre-processing code can be updated any time by going to the code editor using  Pre-processing Code  button under Pre-processing block. We can make our changes, test them by running it. Once finalized, we should  Save  and  Upload  the updated pre-processing code. Project needs to be re-deployed for the new changes to take effect.",
            "title": "Updating Pre-processing Code"
        },
        {
            "location": "/showcase/managemodels/updatemodels/#updating-prediction-code",
            "text": "Prediction code can be updated any time by going to the code editor using  Prediction Code  button under Model block. We can make our changes, test them by running it. Once finalized, we should  Save  and  Upload  the updated prediction code. Project needs to be re-deployed for the new changes to take effect.",
            "title": "Updating Prediction Code"
        },
        {
            "location": "/cli/setup/",
            "text": "Clouderizer offers a python based CLI for creating, deploying and managing its projects.\n\n\nPre-requisites\n\u00b6\n\n\n\n\nSystem running MacOS / Ubuntu / Windows (with WSL)\n\n\nbash / sh / zsh terminal\n\n\nPython 3+\n\n\nPip for python3\n\n\n\n\nSteps\n\u00b6\n\n\n\n\n\n\nInstall Clouderizer CLI by running the following command in the terminal\n\npip install clouderizer\n\n\n\n \n\n\n\n\n\n\nOpen a new terminal session for the changes to take effect. To test if cli was installed, type \ncldz\n. The cldz help section similar to the below image should show up.\n\n\n\n\n\n\n \n\n\n\n\nTo use cli you need to be a Clouderizer user, \ncldz login\n is the command to authenticate yourself.\n\n\n\n\nOnce \ncldz login\n is executed, \n\n\n\u2192 a browser window will open(if browser does not open, manually open the link as seen in image below). \n\n\n \n\n\n\u2192 Follow the Google authentication work flow to get yourself loggedin. If your are an existing user select the google account you are using with clouderizer. New users will be automatically signed up with Clouderizer via the same process.\n\n\n\u2192 After completing the google flow, you are now logged in with clouderizer and redirected to cli login authentication token page. \n\n\n \n\n\n\u2192 Copy the token and paste in the terminal.\n\n\nOn successful login you should see the terminal output similar to below image.\n\n\n \n\n\nCli is successfully setup and you are logged in.",
            "title": "Setup"
        },
        {
            "location": "/cli/setup/#pre-requisites",
            "text": "System running MacOS / Ubuntu / Windows (with WSL)  bash / sh / zsh terminal  Python 3+  Pip for python3",
            "title": "Pre-requisites"
        },
        {
            "location": "/cli/setup/#steps",
            "text": "Install Clouderizer CLI by running the following command in the terminal pip install clouderizer       Open a new terminal session for the changes to take effect. To test if cli was installed, type  cldz . The cldz help section similar to the below image should show up.        To use cli you need to be a Clouderizer user,  cldz login  is the command to authenticate yourself.   Once  cldz login  is executed,   \u2192 a browser window will open(if browser does not open, manually open the link as seen in image below).      \u2192 Follow the Google authentication work flow to get yourself loggedin. If your are an existing user select the google account you are using with clouderizer. New users will be automatically signed up with Clouderizer via the same process.  \u2192 After completing the google flow, you are now logged in with clouderizer and redirected to cli login authentication token page.      \u2192 Copy the token and paste in the terminal.  On successful login you should see the terminal output similar to below image.     Cli is successfully setup and you are logged in.",
            "title": "Steps"
        },
        {
            "location": "/cli/deploy/",
            "text": "Deploy Notebooks\n\u00b6\n\n\nSyntax\n\u00b6\n\n\ncldz deploy -n python PATH_TO_JUPYTER_NOTEBOOK {REQUIREMENTS_TXT}\n\n\n-n\n flag tells \ncldz\n that deployment is of type notebook. Immediately after -n we follow up with the notebook kernel type. Following kernels are supported\n\n\n\n\npython\n\n\nR (coming soon)\n\n\njulia (coming soon)\n\n\n\n\nIf no flag is given, \ncldz\n assumes the first argument is a python based jupyter notebook and second argument is a requirements file.\n\n\nExample\n\u00b6\n\n\ncldz deploy -n python awesome-notebook.ipynb requirements.txt\n\n\nAsync Endpoints\n\u00b6\n\n\nNotebook deployments support Async serverless endpoints. This is useful as often Notebook execution is a long running operation. Async endpoints can be invoked via any http client like curl. Caller can specify a callback URL (webhook) in its http invocation under header \nX-callback-url\n. Serverless function immediately replies back with 202 request accepted response code and continue to execute the notebook. Once notebook execution is complete, the callback URL (webhook) is called with the result. In addition to this webhook, output of the invocation can also be viewed under \nInvocation History\n on Clouderizer console.\n\n\nDeploy Models\n\u00b6\n\n\nSyntax\n\u00b6\n\n\ncldz deploy -m MODEL_TYPE PATH_TO_H2O_MODEL_FILE\n\n\n-m\n flag tells \ncldz\n that deployment is of type model. Immediately after -m we follow up with the model type. Following model types are supported\n\n\n\n\nh2o.ai MOJO model\n\n\nPMML\n\n\nPython\n\n\nONNX\n\n\n\n\nh2o / dai / pmml model deployment\n\u00b6\n\n\ncldz deploy -m h2o PATH_TO_H2O_MODEL_FILE\n\n\n-m\n flag tells \ncldz\n that deployment is of type model.\n\n\nIf \n-m\n is not specified,\ncldz\n assumes type as notebook.\n\n\nh2o in the above example can be replaced with dai / pmml.\n\n\n \n\n\npython model deployment\n\u00b6\n\n\nSince python predict code needs to be executed.\n\n\ncldz deploy -m python PATH_TO_PYTHON_MODEL --predict predict.py\n\n\nPATH_TO_PYTHON_MODEL\n is usually the model file we load and score in \npredict.py\n.\n\n\nPre-processing and Post-processing scripts\n\u00b6\n\n\nDoes your model requires preprocess / postprocess scripts at the time of scoring?\n\n\nSuppose you have a pmml model with both preprocess and postprocess files. Syntax is:\n\n\ncldz deploy -m pmml PATH_TO_PMML_FILE --preprocess preprocess.py --postprocess postprocess.py\n\n\nVERY IMPORTANT NOTE: Preprocess and postprocess flags are optional. Even mispelling the flags will deploy the model but without the mispelled arguments.\n\n\nInfra Configuration Flag\n\u00b6\n\n\n--infra\n flag tells cldz the type of infra you want to run on.\n\n\nClouderizer currently supports 3 infra types:\n\n\n\n\nstandard - 2GB Memory\n\n\nhighmemory - 6GB Memory\n\n\ngpu - 16GB GPU Memory, 30GB System Memory\n\n\n\n\nstandard\n infra type is selected by default but if you want to start a project on gpu \n\n\n\n\nNote\n\n\n\n\nTensorflow 2.4+ is supported for GPU.\n\n\ncldz start PROJECT_NAME --infra gpu\n\n\nImage Configuration Flag\n\u00b6\n\n\n--image\n flag tells cldz the type of base image you want to run on.\n\n\nClouderizer currently supports 3 image types:\n\n\n\n\nstandard - python 3.8\n\n\ntensorflow - python 3.8 + Tensorflow 2.4\n\n\ntorch - python 3.8 + Pytorch + Fastai + nbdev\n\n\n\n\nstandard\n image type is selected by default \n\n\ncldz deploy my_notebook.ipynb --image torch",
            "title": "cldz deploy"
        },
        {
            "location": "/cli/deploy/#deploy-notebooks",
            "text": "",
            "title": "Deploy Notebooks"
        },
        {
            "location": "/cli/deploy/#syntax",
            "text": "cldz deploy -n python PATH_TO_JUPYTER_NOTEBOOK {REQUIREMENTS_TXT}  -n  flag tells  cldz  that deployment is of type notebook. Immediately after -n we follow up with the notebook kernel type. Following kernels are supported   python  R (coming soon)  julia (coming soon)   If no flag is given,  cldz  assumes the first argument is a python based jupyter notebook and second argument is a requirements file.",
            "title": "Syntax"
        },
        {
            "location": "/cli/deploy/#example",
            "text": "cldz deploy -n python awesome-notebook.ipynb requirements.txt",
            "title": "Example"
        },
        {
            "location": "/cli/deploy/#async-endpoints",
            "text": "Notebook deployments support Async serverless endpoints. This is useful as often Notebook execution is a long running operation. Async endpoints can be invoked via any http client like curl. Caller can specify a callback URL (webhook) in its http invocation under header  X-callback-url . Serverless function immediately replies back with 202 request accepted response code and continue to execute the notebook. Once notebook execution is complete, the callback URL (webhook) is called with the result. In addition to this webhook, output of the invocation can also be viewed under  Invocation History  on Clouderizer console.",
            "title": "Async Endpoints"
        },
        {
            "location": "/cli/deploy/#deploy-models",
            "text": "",
            "title": "Deploy Models"
        },
        {
            "location": "/cli/deploy/#syntax_1",
            "text": "cldz deploy -m MODEL_TYPE PATH_TO_H2O_MODEL_FILE  -m  flag tells  cldz  that deployment is of type model. Immediately after -m we follow up with the model type. Following model types are supported   h2o.ai MOJO model  PMML  Python  ONNX",
            "title": "Syntax"
        },
        {
            "location": "/cli/deploy/#h2o-dai-pmml-model-deployment",
            "text": "cldz deploy -m h2o PATH_TO_H2O_MODEL_FILE  -m  flag tells  cldz  that deployment is of type model.  If  -m  is not specified, cldz  assumes type as notebook.  h2o in the above example can be replaced with dai / pmml.",
            "title": "h2o / dai / pmml model deployment"
        },
        {
            "location": "/cli/deploy/#python-model-deployment",
            "text": "Since python predict code needs to be executed.  cldz deploy -m python PATH_TO_PYTHON_MODEL --predict predict.py  PATH_TO_PYTHON_MODEL  is usually the model file we load and score in  predict.py .",
            "title": "python model deployment"
        },
        {
            "location": "/cli/deploy/#pre-processing-and-post-processing-scripts",
            "text": "Does your model requires preprocess / postprocess scripts at the time of scoring?  Suppose you have a pmml model with both preprocess and postprocess files. Syntax is:  cldz deploy -m pmml PATH_TO_PMML_FILE --preprocess preprocess.py --postprocess postprocess.py  VERY IMPORTANT NOTE: Preprocess and postprocess flags are optional. Even mispelling the flags will deploy the model but without the mispelled arguments.",
            "title": "Pre-processing and Post-processing scripts"
        },
        {
            "location": "/cli/deploy/#infra-configuration-flag",
            "text": "--infra  flag tells cldz the type of infra you want to run on.  Clouderizer currently supports 3 infra types:   standard - 2GB Memory  highmemory - 6GB Memory  gpu - 16GB GPU Memory, 30GB System Memory   standard  infra type is selected by default but if you want to start a project on gpu    Note   Tensorflow 2.4+ is supported for GPU.  cldz start PROJECT_NAME --infra gpu",
            "title": "Infra Configuration Flag"
        },
        {
            "location": "/cli/deploy/#image-configuration-flag",
            "text": "--image  flag tells cldz the type of base image you want to run on.  Clouderizer currently supports 3 image types:   standard - python 3.8  tensorflow - python 3.8 + Tensorflow 2.4  torch - python 3.8 + Pytorch + Fastai + nbdev   standard  image type is selected by default   cldz deploy my_notebook.ipynb --image torch",
            "title": "Image Configuration Flag"
        },
        {
            "location": "/cli/update/",
            "text": "Clouderizer cli gives the flexibility to update all the existing project components. \n\n\nHowever converting a project from model \u2192 notebook or notebook \u2192 model is not possible.\n\n\ncldz update\n updates project components.\n\n\n\u2192 update preprocess function\n\u00b6\n\n\ncldz update PROJECT_NAME --preprocess preprocess.py\n\n\nIf a preprocess function already exists in your project, it will be replaced by the function in this file preprocess.py. Else, new preprocess function will be created and enabled. \n\n\n\u2192 add dependencies to your project\n\u00b6\n\n\ncldz update PROJECT_NAME --requirements requirements.txt\n\n\n\u2192 update model\n\u00b6\n\n\ncldz  update PROJECT_NAME --model NEW_MODEL_PATH\n\n\n\u2192 notebook update\n\u00b6\n\n\ncldz  update PROJECT_NAME --notebook NEW_NOTEBOOK_PATH\n\n\nOnce enabled preprocess & postprocess functions can also be disabled.\n\n\n\u2192 disable postprocess function\n\u00b6\n\n\ncldz  update PROJECT_NAME --disable postprocess",
            "title": "cldz update"
        },
        {
            "location": "/cli/update/#-update-preprocess-function",
            "text": "cldz update PROJECT_NAME --preprocess preprocess.py  If a preprocess function already exists in your project, it will be replaced by the function in this file preprocess.py. Else, new preprocess function will be created and enabled.",
            "title": "--&gt; update preprocess function"
        },
        {
            "location": "/cli/update/#-add-dependencies-to-your-project",
            "text": "cldz update PROJECT_NAME --requirements requirements.txt",
            "title": "--&gt; add dependencies to your project"
        },
        {
            "location": "/cli/update/#-update-model",
            "text": "cldz  update PROJECT_NAME --model NEW_MODEL_PATH",
            "title": "--&gt; update model"
        },
        {
            "location": "/cli/update/#-notebook-update",
            "text": "cldz  update PROJECT_NAME --notebook NEW_NOTEBOOK_PATH  Once enabled preprocess & postprocess functions can also be disabled.",
            "title": "--&gt; notebook update"
        },
        {
            "location": "/cli/update/#-disable-postprocess-function",
            "text": "cldz  update PROJECT_NAME --disable postprocess",
            "title": "--&gt; disable postprocess function"
        },
        {
            "location": "/cli/start/",
            "text": "An existing project can be started or restarted with new configuration using \ncldz start\n.\n\n\ncldz start PROJECT_NAME\n\n\nOne flag accepted by \ncldz start\n is the type of --infra you want to run on. \n\n\nClouderizer currently supports 3 infra types:standard, highmemory and gpu.\n\n\nstandard infra type is selected by default but if you want to start a project on gpu \n\n\ncldz start PROJECT_NAME --infra gpu",
            "title": "cldz start"
        },
        {
            "location": "/cli/stop/",
            "text": "To stop a running project, \ncldz stop PROJECT_NAME\n.\n\n\nStopping the projects will make the project URLs inaccessible.",
            "title": "cldz stop"
        },
        {
            "location": "/cli/ls/",
            "text": "You need to be logged in to use this command. Check Cli Setup section.\n\n\ncldz ls\n will give the list of all the deployed projects assosciated with the account.",
            "title": "cldz ls"
        },
        {
            "location": "/cli/describe/",
            "text": "cldz ls\n gives the project status. To get more detailed description of the state of the project \ncldz describe PROJECT_NAME\n.\n\n\nNotebook describe output example,\n\n\n\n\nModel describe output example,\n\n\n \n\n\nIn case of a deployed model, the description contains model access and scoring URLs.",
            "title": "cldz describe"
        },
        {
            "location": "/cli/delete/",
            "text": "To delete a project, \ncldz delete PROJECT_NAME\n.\n\n\nAll the project related files / models / functions / invocation history will be destroyed.\n\u00b6",
            "title": "cldz delete"
        },
        {
            "location": "/cli/delete/#all-the-project-related-files-models-functions-invocation-history-will-be-destroyed",
            "text": "",
            "title": "All the project related files / models / functions / invocation history will be destroyed."
        },
        {
            "location": "/cli/miscellaneous/",
            "text": "cldz loggedin\n\u00b6\n\n\nTo check your user information, execute \ncldz loggedin\n in your terminal.\n\n\n \n\n\ncldz logout\n\u00b6\n\n\nBy default, user will always be logged in with cli(after the first time setup).\n\n\nTo logout, \ncldz logout",
            "title": "Miscellaneous"
        },
        {
            "location": "/cli/miscellaneous/#cldz-loggedin",
            "text": "To check your user information, execute  cldz loggedin  in your terminal.",
            "title": "cldz loggedin"
        },
        {
            "location": "/cli/miscellaneous/#cldz-logout",
            "text": "By default, user will always be logged in with cli(after the first time setup).  To logout,  cldz logout",
            "title": "cldz logout"
        }
    ]
}